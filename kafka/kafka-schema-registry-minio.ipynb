{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb029de",
   "metadata": {},
   "source": [
    "# Kafka Schema Registry MinIO\n",
    "\n",
    "In the previous [Notebook](kafka-minio.ipynb) we saw how to use Kafka Connectors to stream events directly to MinIO but thats the simplest way to stream data which may not be efficient and performant enough for production usecases with large workloads and it is errorprone for instance for a given kafka topic if data chanages like a new column gets added, an existing column gets removed or the data type of a given column gets modified consumer may not be aware of these changes and there is a possibility of corruption.\n",
    "\n",
    "\n",
    "Kafka Schema Registry is a component in the Apache Kafka ecosystem that provides a centralized schema management service for Kafka producers and consumers. It allows producers to register schemas for the data they produce, and consumers to retrieve and use these schemas for data validation and deserialization. The Schema Registry helps ensure that data exchanged through Kafka is compliant with a predefined schema, enabling data consistency, compatibility, and evolution across different systems and applications.\n",
    "\n",
    "Here are some key benefits of using Kafka Schema Registry:\n",
    "\n",
    "* **Schema Evolution**: As data formats and requirements evolve over time, it is common for producers and consumers to undergo changes to their data schemas. Kafka Schema Registry provides support for schema evolution, allowing producers to register new versions of schemas while maintaining compatibility with existing consumers. Consumers can retrieve the appropriate schema version for deserialization, ensuring that data is processed correctly even when schema changes occur\n",
    "* **Data Validation**: Kafka Schema Registry enables data validation by allowing producers to register schemas with predefined data types, field names, and other constraints. Consumers can then retrieve and use these schemas to validate incoming data, ensuring that data conforms to the expected structure and format. This helps prevent data processing errors and improves data quality\n",
    "* **Schema Management**: Kafka Schema Registry provides a centralized repository for managing schemas, making it easier to track, version, and manage changes to data schemas. Producers and consumers can register, retrieve, and manage schemas through a simple API, allowing for centralized schema governance and management.\n",
    "* **Interoperability**: Kafka Schema Registry promotes interoperability between different producers and consumers by providing a standardized way to define and manage data schemas. Producers and consumers written in different programming languages or using different serialization frameworks can use a common schema registry to ensure data consistency and compatibility across the ecosystem\n",
    "* **Backward and Forward Compatibility**: Kafka Schema Registry allows producers to register backward and forward compatible schemas, enabling smooth upgrades and changes to data schemas without disrupting existing producers and consumers. Backward compatibility ensures that older consumers can still process data produced with a newer schema, while forward compatibility allows newer consumers to process data produced with an older schema\n",
    "\n",
    "\n",
    "Strimzi Operator doesn't come with Schema Registry yet we will use the one availabe in confluent helm repository.\n",
    "\n",
    "In this Notebook we will do the following\n",
    "\n",
    "1. Setup Kafka Schema Registry using Helm charts\n",
    "2. Create and deploy a sample producer that uses an Avro schema and sends events\n",
    "3. Build a KafkaConnect continer which has Avro dependency\n",
    "4. Deploy KafkaConnect using the above container\n",
    "5. Deploy KafkaConector that reads the schema from Schema registry, consumes topic events from the producer and stores data into MinIO in parquet format "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0150418",
   "metadata": {},
   "source": [
    "## Setup Schema Registry\n",
    "\n",
    "We will clone the confluent helm repository using the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e923f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/confluentinc/cp-helm-charts.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2e2266",
   "metadata": {},
   "outputs": [],
   "source": [
    "#move to schema registry folder\n",
    "%cd cp-helm-charts/charts/cp-schema-registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311c1a61",
   "metadata": {},
   "source": [
    "Use the blow command to install schema registry using the helm charts, we will need to provide the `bootstrap server endpoint` of the existing kafka cluster we deployed for the installation to be successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a92b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!helm install kafka-schema-registry --set kafka.bootstrapServers=\"PLAINTEXT://my-kafka-cluster-kafka-bootstrap:9092\" . -n kafka\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a06701",
   "metadata": {},
   "source": [
    "You can check if the Schema Registry is up and running successfully by checking the logs as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4cfa3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apr 12, 2023 4:52:25 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime\n",
      "WARNING: A provider io.confluent.kafka.schemaregistry.rest.resources.SchemasResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.kafka.schemaregistry.rest.resources.SchemasResource will be ignored. \n",
      "Apr 12, 2023 4:52:25 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime\n",
      "WARNING: A provider io.confluent.kafka.schemaregistry.rest.resources.SubjectVersionsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.kafka.schemaregistry.rest.resources.SubjectVersionsResource will be ignored. \n",
      "[2023-04-12 16:52:26,511] INFO HV000001: Hibernate Validator 6.1.2.Final (org.hibernate.validator.internal.util.Version)\n",
      "[2023-04-12 16:52:28,268] INFO Started o.e.j.s.ServletContextHandler@3241713e{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)\n",
      "[2023-04-12 16:52:28,378] INFO Started o.e.j.s.ServletContextHandler@7051777c{/ws,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)\n",
      "[2023-04-12 16:52:28,478] INFO Started NetworkTrafficServerConnector@418c5a9c{HTTP/1.1, (http/1.1)}{0.0.0.0:8081} (org.eclipse.jetty.server.AbstractConnector)\n",
      "[2023-04-12 16:52:28,479] INFO Started @18811ms (org.eclipse.jetty.server.Server)\n",
      "[2023-04-12 16:52:28,481] INFO Server started, listening for requests... (io.confluent.kafka.schemaregistry.rest.SchemaRegistryMain)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kafka logs -f --selector=app=cp-schema-registry -c cp-schema-registry-server # stop this shell once you are done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db299f04",
   "metadata": {},
   "source": [
    "### Create Avro Topic\n",
    "\n",
    "Create a YAML file for the kafka topic `nyc-avro-topic` as shown below and apply it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29e2244c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing deployment/kafka-nyc-avro-topic.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile deployment/kafka-nyc-avro-topic.yaml\n",
    "apiVersion: kafka.strimzi.io/v1beta2\n",
    "kind: KafkaTopic\n",
    "metadata:\n",
    "  name: nyc-avro-topic\n",
    "  namespace: kafka\n",
    "  labels:\n",
    "    strimzi.io/cluster: my-kafka-cluster\n",
    "spec:\n",
    "  partitions: 3\n",
    "  replicas: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f972526b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kafkatopic.kafka.strimzi.io/nyc-avro-topic created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f deployment/kafka-nyc-avro-topic.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a38d7e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME             CLUSTER            PARTITIONS   REPLICATION FACTOR   READY\r\n",
      "nyc-avro-topic   my-kafka-cluster   3            3                    True\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kafka get kafkatopic nyc-avro-topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ba2f99",
   "metadata": {},
   "source": [
    "## Producer with Avro Schema\n",
    "\n",
    "We will create a simple python producer than register Avro schema with the Kafka Schema Registry and sends kafka topic events. This will be based on the producer that we already had in the previous [Notebook](kafka-minio.ipynb#Producer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7d99b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sample-code/producer/src/avro-producer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sample-code/producer/src/avro-producer.py\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import fsspec\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "from avro.schema import make_avsc_object\n",
    "from confluent_kafka.avro import AvroProducer\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Avro schema\n",
    "value_schema_dict = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"nyc_avro\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"VendorID\",\n",
    "            \"type\": \"long\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"tpep_pickup_datetime\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"tpep_dropoff_datetime\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"passenger_count\",\n",
    "            \"type\": \"double\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"trip_distance\",\n",
    "            \"type\": \"double\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"RatecodeID\",\n",
    "            \"type\": \"double\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"store_and_fwd_flag\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"PULocationID\",\n",
    "            \"type\": \"long\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"DOLocationID\",\n",
    "            \"type\": \"long\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"payment_type\",\n",
    "            \"type\": \"long\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"fare_amount\",\n",
    "            \"type\": \"double\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"extra\",\n",
    "            \"type\": \"double\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"mta_tax\",\n",
    "            \"type\": \"double\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"tip_amount\",\n",
    "            \"type\": \"double\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"tolls_amount\",\n",
    "            \"type\": \"double\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"improvement_surcharge\",\n",
    "            \"type\": \"double\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"total_amount\",\n",
    "            \"type\": \"double\"\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "value_schema = make_avsc_object(value_schema_dict)\n",
    "\n",
    "producer_config = {\n",
    "    \"bootstrap.servers\": \"my-kafka-cluster-kafka-bootstrap:9092\",\n",
    "    \"schema.registry.url\": \"http://kafka-schema-registry-cp-schema-registry:8081\"\n",
    "}\n",
    "\n",
    "producer = AvroProducer(producer_config, default_value_schema=value_schema)\n",
    "\n",
    "fsspec.config.conf = {\n",
    "    \"s3\":\n",
    "        {\n",
    "            \"key\": os.getenv(\"AWS_ACCESS_KEY_ID\", \"openlakeuser\"),\n",
    "            \"secret\": os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"openlakeuser\"),\n",
    "            \"client_kwargs\": {\n",
    "                \"endpoint_url\": \"https://play.min.io:50000\"\n",
    "            }\n",
    "        }\n",
    "}\n",
    "s3 = s3fs.S3FileSystem()\n",
    "total_processed = 0\n",
    "i = 1\n",
    "for df in pd.read_csv('s3a://openlake/spark/sample-data/taxi-data.csv', chunksize=10000):\n",
    "    count = 0\n",
    "    for index, row in df.iterrows():\n",
    "        producer.produce(topic=\"nyc-avro-topic\", value=row.to_dict())\n",
    "        count += 1\n",
    "\n",
    "    total_processed += count\n",
    "    if total_processed % 10000 * i == 0:\n",
    "        producer.flush()\n",
    "        logging.info(f\"total processed till now {total_processed} for topic 'nyc-avro-topic'\")\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e15261b",
   "metadata": {},
   "source": [
    "add requirements and Dockerfile based on which we will build the docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9624f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sample-code/producer/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile sample-code/producer/requirements.txt\n",
    "pandas==2.0.0\n",
    "s3fs==2023.4.0\n",
    "pyarrow==11.0.0\n",
    "kafka-python==2.0.2\n",
    "confluent_kafka[avro]==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a049ac22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sample-code/producer/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile sample-code/producer/Dockerfile\n",
    "FROM python:3.11-slim\n",
    "\n",
    "ENV PYTHONDONTWRITEBYTECODE=1\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip3 install -r requirements.txt\n",
    "\n",
    "COPY src/avro-producer.py .\n",
    "CMD [\"python3\", \"-u\", \"./avro-producer.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10bc9aa",
   "metadata": {},
   "source": [
    "Build and push the docker image for the producer using the above docker file into your docker registry or you can use the one available in openlake [openlake/kafka-demo-avro-producer](https://hub.docker.com/r/openlake/kafka-demo-avro-producer/tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ef0738",
   "metadata": {},
   "source": [
    "Let's create a YAML that deploys our producer in kubernetes cluster as a job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e612547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing deployment/avro-producer.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile deployment/avro-producer.yaml\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: avro-producer-job\n",
    "  namespace: kafka\n",
    "spec:\n",
    "  template:\n",
    "    metadata:\n",
    "      name: avro-producer-job\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: avro-producer-job\n",
    "        image: openlake/kafka-demo-avro-producer:latest\n",
    "      restartPolicy: Never"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4bf6fc",
   "metadata": {},
   "source": [
    "Deploy the `avro-producer.yaml` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66ea82f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job.batch/avro-producer-job created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f deployment/avro-producer.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f58c9e8",
   "metadata": {},
   "source": [
    "You can check the logs by using the below command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "668f3584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server (NotFound): jobs.batch \"avro-producer-job\" not found\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs -f job.batch/avro-producer-job -n kafka # stop this shell once you are done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7889b44",
   "metadata": {},
   "source": [
    "### Build Kafka Connect Image\n",
    "\n",
    "Lets build a kafka connect image that has S3 and Avro dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31a22fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sample-code/connect/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile sample-code/connect/Dockerfile\n",
    "FROM confluentinc/cp-kafka-connect:7.0.9 as cp\n",
    "RUN confluent-hub install --no-prompt confluentinc/kafka-connect-s3:10.4.2\n",
    "RUN confluent-hub install --no-prompt confluentinc/kafka-connect-avro-converter:7.3.3\n",
    "FROM quay.io/strimzi/kafka:0.34.0-kafka-3.4.0\n",
    "USER root:root\n",
    "# Add S3 dependency\n",
    "COPY --from=cp /usr/share/confluent-hub-components/confluentinc-kafka-connect-s3/ /opt/kafka/plugins/kafka-connect-s3/\n",
    "# Add Avro dependency\n",
    "COPY --from=cp /usr/share/confluent-hub-components/confluentinc-kafka-connect-avro-converter/ /opt/kafka/plugins/avro/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c7ef3e",
   "metadata": {},
   "source": [
    "Build and push the docker image for the producer using the above docker file into your docker registry or you can use the one available in openlake [openlake/kafka-connect:0.34.0](https://hub.docker.com/r/openlake/kafka-connect/tags)\n",
    "\n",
    "Before we deploy the `KafkaConnect` we first need to create storage topcis if not already present for the KafkaConnect to work as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6b49aa",
   "metadata": {},
   "source": [
    "### Deploy Kafka Connect\n",
    "\n",
    "Create a YAML file for Kafka Connect that uses the above image and deploy it in k8s. The KafkaConnect will have 1 replica and make use of ths storage topics that we created in the previous [Notebook](kafka-minio.ipynb#Create-Storage-Topics).\n",
    "\n",
    "NOTE: `spec.template.connectContainer.env` has the creds defiend in order for KafkaConnect to store data in Minio cluster, other details like the `endpoint_url`, `bucket_name` will be part of `KafkaConnector`. `key.converter` and `value.converter` is pointing to AvroConverter (`io.confluent.connect.avro.AvroConverter`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea6dd12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting deployment/avro-connect.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile deployment/avro-connect.yaml\n",
    "apiVersion: kafka.strimzi.io/v1beta2\n",
    "kind: KafkaConnect\n",
    "metadata:\n",
    "  name: avro-connect-cluster\n",
    "  namespace: kafka\n",
    "  annotations:\n",
    "    strimzi.io/use-connector-resources: \"true\"\n",
    "spec:\n",
    "  image: openlake/kafka-connect:0.34.0\n",
    "  version: 3.4.0\n",
    "  replicas: 1\n",
    "  bootstrapServers: my-kafka-cluster-kafka-bootstrap:9093\n",
    "  tls:\n",
    "    trustedCertificates:\n",
    "      - secretName: my-kafka-cluster-cluster-ca-cert\n",
    "        certificate: ca.crt\n",
    "  config:\n",
    "    bootstrap.servers: my-kafka-cluster-kafka-bootstrap:9092\n",
    "    group.id: avro-connect-cluster\n",
    "    key.converter: io.confluent.connect.avro.AvroConverter\n",
    "    value.converter: io.confluent.connect.avro.AvroConverter\n",
    "    internal.key.converter: org.apache.kafka.connect.json.JsonConverter\n",
    "    internal.value.converter: org.apache.kafka.connect.json.JsonConverter\n",
    "    key.converter.schemas.enable: false\n",
    "    value.converter.schemas.enable: false\n",
    "    offset.storage.topic: connect-offsets\n",
    "    offset.storage.replication.factor: 1\n",
    "    config.storage.topic: connect-configs\n",
    "    config.storage.replication.factor: 1\n",
    "    status.storage.topic: connect-status\n",
    "    status.storage.replication.factor: 1\n",
    "    offset.flush.interval.ms: 10000\n",
    "    plugin.path: /opt/kafka/plugins\n",
    "    offset.storage.file.filename: /tmp/connect.offsets\n",
    "  template:\n",
    "    connectContainer:\n",
    "      env:\n",
    "        - name: AWS_ACCESS_KEY_ID\n",
    "          value: \"openlakeuser\"\n",
    "        - name: AWS_SECRET_ACCESS_KEY\n",
    "          value: \"openlakeuser\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eeb411f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kafkaconnect.kafka.strimzi.io/avro-connect-cluster created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f deployment/avro-connect.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e66394a",
   "metadata": {},
   "source": [
    "### Deploy Kafka Sink Connector\n",
    "\n",
    "Now that we have the Kafka Connect up and running next step is to deploy the sink connector that will poll `nyc-avro-topic` and store the data into MinIO bucket `openlake-tmp` in `parquet` format.\n",
    "\n",
    "\n",
    "`connector.class` - specifies what type of connector the sink connector will use in our case it is `io.confluent.connect.s3.S3SinkConnector`\n",
    "\n",
    "`store.url` - MinIO endpoint URL where you want to store the data from KafkaConnect\n",
    "\n",
    "`storage.class` - specifies which storage class to use in our case since we are storing in MinIO `io.confluent.connect.s3.storage.S3Storage` will be used\n",
    "\n",
    "`format.class` -  Format type in which the data will be stored into MinIO, since we would like to store `parquet` we will use `io.confluent.connect.s3.format.parquet.ParquetFormat` implementation\n",
    "\n",
    "`value.converter` - Since we want to convert the binary data to `avro` we will use `io.confluent.connect.avro.AvroConverter`\n",
    "\n",
    "`parquet.codec` - Specifies what type of compression we would like to use for the parquet files, in our case we will use `snappy`\n",
    "\n",
    "`schema.registry.url` -  Specifies the endpoint from which the connector can pull, validate the schema and deserialize the data from the producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2511cb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting deployment/avro-connector.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile deployment/avro-connector.yaml\n",
    "apiVersion: kafka.strimzi.io/v1beta2\n",
    "kind: KafkaConnector\n",
    "metadata:\n",
    "  name: \"avro-connector\"\n",
    "  namespace: \"kafka\"\n",
    "  labels:\n",
    "    strimzi.io/cluster:\n",
    "      avro-connect-cluster\n",
    "spec:\n",
    "  class: io.confluent.connect.s3.S3SinkConnector\n",
    "  config:\n",
    "    connector.class: io.confluent.connect.s3.S3SinkConnector\n",
    "    task.max: '1'\n",
    "    topics: nyc-avro-topic\n",
    "    s3.region: us-east-1\n",
    "    s3.bucket.name: openlake-tmp\n",
    "    s3.part.size: '5242880'\n",
    "    flush.size: '10000'\n",
    "    topics.dir: nyc-taxis-avro\n",
    "    timezone: UTC\n",
    "    store.url: https://play.min.io:50000\n",
    "    storage.class: io.confluent.connect.s3.storage.S3Storage\n",
    "    format.class: io.confluent.connect.s3.format.parquet.ParquetFormat\n",
    "    partitioner.class: io.confluent.connect.storage.partitioner.DefaultPartitioner\n",
    "    s3.credentials.provider.class: com.amazonaws.auth.DefaultAWSCredentialsProviderChain\n",
    "    behavior.on.null.values: ignore\n",
    "    auto.register.schemas: false\n",
    "    parquet.codec: snappy\n",
    "    schema.registry.url: http://kafka-schema-registry-cp-schema-registry:8081\n",
    "    value.converter: io.confluent.connect.avro.AvroConverter\n",
    "    key.converter: org.apache.kafka.connect.storage.StringConverter\n",
    "    value.converter.schema.registry.url: http://kafka-schema-registry-cp-schema-registry:8081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b7cb521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kafkaconnector.kafka.strimzi.io/avro-connector created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f deployment/avro-connector.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b040af8",
   "metadata": {},
   "source": [
    "If all goes well we can see files being added to Minio `openlake-tmp` bucket by executing the below command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "925bbaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b]11;?\u001b\\\u001b[6n\u001b[m\u001b[32m[2023-04-12 12:45:31 PDT]\u001b[0m\u001b[33m 167KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/nyc-avro-topic+0+0000000000.snappy.parquet\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-12 12:45:33 PDT]\u001b[0m\u001b[33m 187KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/nyc-avro-topic+0+0000010000.snappy.parquet\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-12 12:45:34 PDT]\u001b[0m\u001b[33m 179KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/nyc-avro-topic+0+0000020000.snappy.parquet\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-12 12:45:35 PDT]\u001b[0m\u001b[33m 167KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/nyc-avro-topic+0+0000030000.snappy.parquet\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-12 12:45:36 PDT]\u001b[0m\u001b[33m 178KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/nyc-avro-topic+0+0000040000.snappy.parquet\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-12 12:45:37 PDT]\u001b[0m\u001b[33m 179KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/nyc-avro-topic+0+0000050000.snappy.parquet\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-12 12:45:38 PDT]\u001b[0m\u001b[33m 165KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/nyc-avro-topic+0+0000060000.snappy.parquet\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-12 12:45:40 PDT]\u001b[0m\u001b[33m 187KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/nyc-avro-topic+0+0000070000.snappy.parquet\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-12 12:45:41 PDT]\u001b[0m\u001b[33m 167KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/nyc-avro-topic+0+0000080000.snappy.parquet\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-12 12:45:42 PDT]\u001b[0m\u001b[33m 188KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/nyc-avro-topic+0+0000090000.snappy.parquet\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-12 12:45:43 PDT]\u001b[0m\u001b[33m 191KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/nyc-avro-topic+0+0000100000.snappy.parquet\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[1m\n",
      "Total Size: 1.9 MiB\u001b[0m\n",
      "\u001b[1mTotal Objects: 11\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!mc ls --summarize --recursive play/openlake-tmp/nyc-taxis-avro/nyc-avro-topic/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca976ddd",
   "metadata": {},
   "source": [
    "The current setup that we have is significantly faster and highly storage efficient that the previous basic setup that we had in the previous [Notebook](kafka-minio.ipynb), you can try running both the Producers and connectors and see the peformance and memory differences.\n",
    "\n",
    "With this we have an end-to-end setup for efficiently for producing data kafka topics using Avro schema and consuming it directly into MinIO in parquet format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dfe09c",
   "metadata": {},
   "source": [
    "# Experimental: Iceberg\n",
    "\n",
    "Recently Iceberg connector support has been added to kafka by `getindata` [here](https://github.com/getindata/kafka-connect-iceberg-sink) is the repo. Below we will explore how to store the `nyc-avro-topic` data directly as Iceberg table into MinIO. This is still experimental and not ready for production IMO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b90b754",
   "metadata": {},
   "source": [
    "## Iceberg Kafka Connect\n",
    "\n",
    "Let's create a KafkaConnect that has Iceberg dependencies, make sure to edit the `spec.config.build.output.image` and `spec.config.build.output.pushSecret` to point to your Docker Registry before deploying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "545c7c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting deployment/iceberg-connect.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile deployment/iceberg-connect.yaml\n",
    "apiVersion: kafka.strimzi.io/v1beta2\n",
    "kind: KafkaConnect\n",
    "metadata:\n",
    "    name: iceberg-connect-cluster\n",
    "    namespace: \"kafka\"\n",
    "    annotations:\n",
    "        strimzi.io/use-connector-resources: \"true\"\n",
    "spec:\n",
    "    version: 3.4.0\n",
    "    replicas: 1\n",
    "    bootstrapServers: my-kafka-cluster-kafka-bootstrap:9093\n",
    "    tls:\n",
    "        trustedCertificates:\n",
    "            -   secretName: my-kafka-cluster-cluster-ca-cert\n",
    "                certificate: ca.crt\n",
    "    logging:\n",
    "        type: inline\n",
    "        loggers:\n",
    "            log4j.rootLogger: \"ERROR\"\n",
    "            log4j.logger.com.getindata.kafka.connect.iceberg.sink.IcebergSinkTask: \"DEBUG\"\n",
    "            log4j.logger.org.apache.hadoop.io.compress.CodecPool: \"WARN\"\n",
    "    config:\n",
    "        group.id: iceberg-connect-cluster\n",
    "        offset.storage.topic: connect-offsets\n",
    "        config.storage.topic: connect-configs\n",
    "        status.storage.topic: connect-status\n",
    "        config.storage.replication.factor: 1\n",
    "        offset.storage.replication.factor: 1\n",
    "        status.storage.replication.factor: 1\n",
    "        config.providers: file,secret,configmap\n",
    "        config.providers.file.class: org.apache.kafka.common.config.provider.FileConfigProvider\n",
    "        config.providers.secret.class: io.strimzi.kafka.KubernetesSecretConfigProvider\n",
    "        config.providers.configmap.class: io.strimzi.kafka.KubernetesConfigMapConfigProvider\n",
    "        bootstrap.servers: my-kafka-cluster-kafka-bootstrap:9092\n",
    "        key.converter: io.confluent.connect.avro.AvroConverter\n",
    "        value.converter: io.confluent.connect.avro.AvroConverter\n",
    "        internal.key.converter: org.apache.kafka.connect.json.JsonConverter\n",
    "        internal.value.converter: org.apache.kafka.connect.json.JsonConverter\n",
    "        key.converter.schemas.enable: false\n",
    "        value.converter.schemas.enable: false\n",
    "        plugin.path: /opt/kafka/plugins\n",
    "        offset.storage.file.filename: /tmp/connect.offsets\n",
    "        build:\n",
    "            output:\n",
    "                type: docker\n",
    "                image: <NameOfYourRegistry>\n",
    "                pushSecret: <RegistrySecret>\n",
    "            plugins:\n",
    "                -   name: kafka-avro-converter\n",
    "                    artifacts:\n",
    "                        -   type: zip\n",
    "                            url: https://d1i4a15mxbxib1.cloudfront.net/api/plugins/confluentinc/kafka-connect-avro-converter/versions/7.3.1/confluentinc-kafka-connect-avro-converter-7.3.1.zip\n",
    "                -   name: iceberg\n",
    "                    artifacts:\n",
    "                        -   type: zip\n",
    "                            url: https://github.com/getindata/kafka-connect-iceberg-sink/releases/download/0.3.1/kafka-connect-iceberg-sink-0.3.1-plugin.zip\n",
    "    resources:\n",
    "        requests:\n",
    "            cpu: \"0.1\"\n",
    "            memory: 512Mi\n",
    "        limits:\n",
    "            cpu: \"5\"\n",
    "            memory: 15Gi\n",
    "    template:\n",
    "        connectContainer:\n",
    "            env:\n",
    "                # important for using AWS s3 client sdk\n",
    "                -   name: AWS_REGION\n",
    "                    value: \"us-east-1\"\n",
    "                -   name: AWS_ACCESS_KEY_ID\n",
    "                    value: \"openlakeuser\"\n",
    "                -   name: AWS_SECRET_ACCESS_KEY\n",
    "                    value: \"openlakeuser\"\n",
    "                -   name: S3_ENDPOINT\n",
    "                    value: \"https://play.min.io:50000\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ff999ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kafkaconnect.kafka.strimzi.io/iceberg-connect-cluster created\r\n"
     ]
    }
   ],
   "source": [
    "# Deploy the above KafkaConnect CRD\n",
    "!kubectl apply -f deployment/iceberg-connect.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20383740",
   "metadata": {},
   "source": [
    "## Deploy Iceberg Sink Connector\n",
    "\n",
    "Now that we have the Iceberg KafkaConnect deployed lets deploy the KafkaConnector that will store Iceberg table directly into MinIO. There are 3 possible Connectors that you can use as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4318d5ff",
   "metadata": {},
   "source": [
    "### Hadoop Iceberg Sink Connector\n",
    "Below deployment will use the `Hadoop catalog` to create and maintain Iceberg table in MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40fb155d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting deployment/iceberg-hadoop-connector.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile deployment/iceberg-hadoop-connector.yaml\n",
    "apiVersion: kafka.strimzi.io/v1beta2\n",
    "kind: KafkaConnector\n",
    "metadata:\n",
    "    name: iceberg-hadoop-sink-connector\n",
    "    namespace: kafka\n",
    "    labels:\n",
    "        strimzi.io/cluster: my-connect-cluster\n",
    "    annotations:\n",
    "        strimzi.io/restart: \"true\"\n",
    "spec:\n",
    "    class: com.getindata.kafka.connect.iceberg.sink.IcebergSink\n",
    "    tasksMax: 1\n",
    "    config:\n",
    "        task.max: '1'\n",
    "        topics: nyc-avro-topic\n",
    "        timezone: UTC\n",
    "        schema.compatibility: NONE\n",
    "        behavior.on.null.values: ignore\n",
    "        auto.register.schemas: true\n",
    "        schema.registry.url: http://kafka-schema-registry-cp-schema-registry:8081\n",
    "        value.converter: io.confluent.connect.avro.AvroConverter\n",
    "        key.converter: org.apache.kafka.connect.storage.StringConverter\n",
    "        partitioner.class: io.confluent.connect.storage.partitioner.DefaultPartitioner\n",
    "        value.converter.schema.registry.url: http://kafka-schema-registry-cp-schema-registry:8081\n",
    "        table.namespace: \"kafka\"\n",
    "        table.prefix: \"\"\n",
    "        table.auto-create: true\n",
    "        table.write-format: \"parquet\"\n",
    "        iceberg.catalog.default.type: hive\n",
    "        iceberg.catalog-impl: \"org.apache.iceberg.hadoop.HadoopCatalog\"\n",
    "        iceberg.table-default.write.metadata.delete-after-commit.enabled: true\n",
    "        iceberg.table-default.write.metadata.previous-versions-max: 10\n",
    "        iceberg.table-default.write.merge.mode: \"merge-on-read\"\n",
    "        iceberg.table-default.write.delete.mode: \"merge-on-read\"\n",
    "        iceberg.table-default.commit.manifest.min-count-to-merge: 5\n",
    "        iceberg.catalog-name: mycatalog\n",
    "        iceberg.warehouse: \"s3a://opentable-tmp/warehouse/nyc\"\n",
    "        iceberg.fs.defaultFS: \"s3a://opentable-tmp\"\n",
    "        iceberg.fs.s3a.path.style.access: true\n",
    "        iceberg.fs.s3a.fast.upload: true\n",
    "        iceberg.fs.s3a.fast.upload.buffer: \"bytebuffer\"\n",
    "        iceberg.fs.s3a.endpoint: https://play.min.io:50000\n",
    "        iceberg.fs.s3a.impl: \"org.apache.hadoop.fs.s3a.S3AFileSystem\"\n",
    "        iceberg.fs.s3a.access.key: 'openlakeuser'\n",
    "        iceberg.fs.s3a.secret.key: 'openlakeuser'\n",
    "        iceberg.fs.s3a.connection.ssl.enabled: true\n",
    "        receive.buffer.bytes: 20485760\n",
    "        fetch.max.bytes: 52428800\n",
    "        consumer.override.max.poll.records: 10000\n",
    "        offset.storage.file.filename: /tmp/connect.offsets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2a6abe",
   "metadata": {},
   "source": [
    "### Hive Iceberg Sink Connector\n",
    "Below deployment will use the `Hive catalog` to create and maintain Iceberg table in MinIO\n",
    "\n",
    "Note: `iceberg.uri`, `iceberg.catalog-impl`, `iceberg.table-default.write.data.path`, `iceberg.table-default.write.metadata.path` are required for Iceberg Hive catalog work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4da7bb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing deployment/iceberg-hive-connector.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile deployment/iceberg-hive-connector.yaml\n",
    "apiVersion: kafka.strimzi.io/v1beta2\n",
    "kind: KafkaConnector\n",
    "metadata:\n",
    "    name: iceberg-hive-sink-connector\n",
    "    namespace: kafka\n",
    "    labels:\n",
    "        strimzi.io/cluster: my-connect-cluster\n",
    "    annotations:\n",
    "        strimzi.io/restart: \"true\"\n",
    "spec:\n",
    "    class: com.getindata.kafka.connect.iceberg.sink.IcebergSink\n",
    "    tasksMax: 1\n",
    "    config:\n",
    "        task.max: '1'\n",
    "        topics: nyc-avro-topic\n",
    "        timezone: UTC\n",
    "        schema.compatibility: NONE\n",
    "        behavior.on.null.values: ignore\n",
    "        auto.register.schemas: true\n",
    "        schema.registry.url: http://kafka-schema-registry-cp-schema-registry:8081\n",
    "        value.converter: io.confluent.connect.avro.AvroConverter\n",
    "        key.converter: org.apache.kafka.connect.storage.StringConverter\n",
    "        partitioner.class: io.confluent.connect.storage.partitioner.DefaultPartitioner\n",
    "        value.converter.schema.registry.url: http://kafka-schema-registry-cp-schema-registry:8081\n",
    "        table.namespace: \"kafka\"\n",
    "        table.prefix: \"\"\n",
    "        table.auto-create: true\n",
    "        table.write-format: \"parquet\"\n",
    "        iceberg.catalog.default.type: hive\n",
    "        iceberg.uri: thrift://metastore-svc:9083 # required for Hive catalog to work\n",
    "        iceberg.catalog-impl: \"org.apache.iceberg.hive.HiveCatalog\"\n",
    "        iceberg.table-default.write.data.path: \"s3a://openlake-tmp/warehouse/nyc/nyc-taxi-data\"\n",
    "        iceberg.table-default.write.metadata.path: \"s3a://openlake-tmp/warehouse/nyc/nyc-taxi-data/metadata\"\n",
    "        iceberg.table-default.write.metadata.delete-after-commit.enabled: true\n",
    "        iceberg.io-impl: \"org.apache.iceberg.aws.s3.S3FileIO\"\n",
    "        iceberg.engine.hive.enabled: true\n",
    "        iceberg.catalog-name: mycatalog\n",
    "        iceberg.catalog.default.catalog-impl: org.apache.iceberg.hive.HiveCatalog\n",
    "        iceberg.warehouse: \"s3a://opentable-tmp/warehouse/nyc\"\n",
    "        iceberg.fs.defaultFS: \"s3a://opentable-tmp\"\n",
    "        iceberg.fs.s3a.path.style.access: true\n",
    "        iceberg.fs.s3a.fast.upload: true\n",
    "        iceberg.fs.s3a.fast.upload.buffer: \"bytebuffer\"\n",
    "        iceberg.fs.s3a.endpoint: https://play.min.io:50000\n",
    "        iceberg.fs.s3a.access.key: 'openlakeuser'\n",
    "        iceberg.fs.s3a.secret.key: 'openlakeuser'\n",
    "        iceberg.fs.s3a.connection.ssl.enabled: true\n",
    "        receive.buffer.bytes: 20485760\n",
    "        fetch.max.bytes: 52428800\n",
    "        consumer.override.max.poll.records: 10000\n",
    "        offset.storage.file.filename: /tmp/connect.offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558480a5",
   "metadata": {},
   "source": [
    "### Nessie Iceberg Sink Connector\n",
    "Below deployment will use the `Nessie catalog` to create and maintain Iceberg table in MinIO.\n",
    "\n",
    "Note: `iceberg.uri`, `iceberg.ref`, `iceberg.catalog-impl` are the key things that change to make Iceberg Nessie catalog work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3dcd07b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing deployment/iceberg-nessie-connector.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile deployment/iceberg-nessie-connector.yaml\n",
    "apiVersion: kafka.strimzi.io/v1beta2\n",
    "kind: KafkaConnector\n",
    "metadata:\n",
    "    name: iceberg-nessie-sink-connector\n",
    "    namespace: kafka\n",
    "    labels:\n",
    "        strimzi.io/cluster: my-connect-cluster\n",
    "    annotations:\n",
    "        strimzi.io/restart: \"true\"\n",
    "spec:\n",
    "    class: com.getindata.kafka.connect.iceberg.sink.IcebergSink\n",
    "    tasksMax: 1\n",
    "    config:\n",
    "        task.max: '1'\n",
    "        topics: nyc-avro-topic\n",
    "        timezone: UTC\n",
    "        schema.compatibility: NONE\n",
    "        behavior.on.null.values: ignore\n",
    "        auto.register.schemas: true\n",
    "        schema.registry.url: http://kafka-schema-registry-cp-schema-registry:8081\n",
    "        value.converter: io.confluent.connect.avro.AvroConverter\n",
    "        key.converter: org.apache.kafka.connect.storage.StringConverter\n",
    "        partitioner.class: io.confluent.connect.storage.partitioner.DefaultPartitioner\n",
    "        value.converter.schema.registry.url: http://kafka-schema-registry-cp-schema-registry:8081\n",
    "        table.namespace: \"kafka\"\n",
    "        table.prefix: \"\"\n",
    "        table.auto-create: true\n",
    "        table.write-format: \"parquet\"\n",
    "        iceberg.uri: http://nessie.nessie-ns.svc:19120/api/v1 # required for Nessie catalog to work\n",
    "        iceberg.ref: \"dev\"  # required for Nessie (branch name)\n",
    "        iceberg.catalog-impl: \"org.apache.iceberg.nessie.NessieCatalog\"\n",
    "        iceberg.table-default.write.metadata.delete-after-commit.enabled: true\n",
    "        iceberg.io-impl: \"org.apache.iceberg.aws.s3.S3FileIO\"\n",
    "        iceberg.engine.hive.enabled: true\n",
    "        iceberg.catalog-name: mycatalog\n",
    "        iceberg.catalog.default.catalog-impl: org.apache.iceberg.hive.HiveCatalog\n",
    "        iceberg.warehouse: \"s3a://opentable-tmp/warehouse/nyc\"\n",
    "        iceberg.fs.defaultFS: \"s3a://opentable-tmp\"\n",
    "        iceberg.fs.s3a.path.style.access: true\n",
    "        iceberg.fs.s3a.fast.upload: true\n",
    "        iceberg.fs.s3a.fast.upload.buffer: \"bytebuffer\"\n",
    "        iceberg.fs.s3a.endpoint: https://play.min.io:50000\n",
    "        iceberg.fs.s3a.access.key: 'openlakeuser'\n",
    "        iceberg.fs.s3a.secret.key: 'openlakeuser'\n",
    "        iceberg.fs.s3a.connection.ssl.enabled: true\n",
    "        receive.buffer.bytes: 20485760\n",
    "        fetch.max.bytes: 52428800\n",
    "        consumer.override.max.poll.records: 10000\n",
    "        offset.storage.file.filename: /tmp/connect.offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488faf70",
   "metadata": {},
   "source": [
    "Use any of the following command to deploy the KafkaConnector with the Iceberg Catalog of your choice, by default hadoop catalog has be enabled below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d7e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f deployment/iceberg-hadoop-connector.yaml\n",
    "# !kubectl apply -f deployment/iceberg-hive-connector.yaml\n",
    "# !kubectl apply -f deployment/iceberg-nessie-connector.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f1791b",
   "metadata": {},
   "source": [
    "This brings us to the endof this Notebook, with the above steps you should have end-to-end setup to stream data from Kafka to MinIO as Iceberg table directly. As mentioned earlier Iceberg connector for Kafka is experimental based on the initial experiments that I performed and not yet ready for production, this could change soon as there is active development going on. If you have Spark already setup and would like a production ready solution for storing Iceberg tables in MinIO you can explore Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c41a4be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
