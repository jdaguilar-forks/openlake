{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adf446ed",
   "metadata": {},
   "source": [
    "# MinIO Kafka Connector\n",
    "\n",
    "\n",
    "In the previous [Notebook](setup-kafka.ipynb) we saw how to setup Kafka in a kubernetes cluster using Strimzi Opearator, in this notebook we will use Kafka connector to stream topics directly to MinIO. First let's look at what connectors are and how to set one up. Here is an high level overview of all the different Kafka Components and how they interact\n",
    "\n",
    "![kafka_components](./img/kafka_components.png)\n",
    "\n",
    "## Kafka Connectors\n",
    "\n",
    "Kafka Connect is an integration toolkit for streaming data between Kafka brokers and other systems. The other system is typically an external data source or target, such as a MinIO.\n",
    "\n",
    "Kafka Connect utilizes a plugin architecture to provide implementation artifacts for connectors, which are used for connecting to external systems and manipulating data. Plugins consist of connectors, data converters, and transforms. Connectors are designed to work with specific external systems and define a schema for their configuration. When configuring Kafka Connect, you supply the necessary configuration to create a connector instance within Kafka Connect, and connector instances then define a set of tasks for data movement between systems.\n",
    "\n",
    "In the distributed mode of operation, Strimzi operates Kafka Connect by distributing data streaming tasks across one or more worker pods. A Kafka Connect cluster consists of a group of worker pods, with each connector instantiated on a single worker. Each connector can have one or more tasks that are distributed across the group of workers, enabling highly scalable data pipelines.\n",
    "\n",
    "Workers in Kafka Connect are responsible for converting data from one format to another, making it suitable for the source or target system. Depending on the configuration of the connector instance, workers may also apply transforms, also known as Single Message Transforms (SMTs), which can adjust messages, such as filtering certain data, before they are converted. Kafka Connect comes with some built-in transforms, but additional transformations can be provided by plugins as needed.\n",
    "\n",
    "\n",
    "Kafka Connect uses the following components while streaming data\n",
    "\n",
    "* Connectors - create tasks\n",
    "* Task - move data\n",
    "* Workers - run tasks\n",
    "* Transformers - manipulate data\n",
    "* Converters - convert data\n",
    "\n",
    "There are 2 types of Connectors\n",
    "\n",
    "1. Source Connectors - push data into Kafka\n",
    "2. Sink Connectors - extracts data from Kafka to external source like MinIO\n",
    "\n",
    "In this Notebook we will focus on Sink Connector that extracts data from Kafka and stores it into MinIO as shown below\n",
    "\n",
    "![kafka_sink_connector](./img/sink_connector_streaming_minio.png)\n",
    "\n",
    "\n",
    "Sink Connector streams data from Kafka and goes through following steps\n",
    "\n",
    "1. A plugin provides the implementation artifacts for the sink connector: In Kafka Connect, a sink connector is used to stream data from Kafka to an external data system. The implementation artifacts for the sink connector, such as the code and configuration, are provided by a plugin. Plugins are used to extend the functionality of Kafka Connect and enable connections to different external data systems.\n",
    "2. A single worker initiates the sink connector instance: In a distributed mode of operation, Kafka Connect runs as a cluster of worker pods. Each worker pod can initiate a sink connector instance, which is responsible for streaming data from Kafka to the external data system. The worker manages the lifecycle of the sink connector instance, including its initialization and configuration.\n",
    "3. The sink connector creates tasks to stream data: Once the sink connector instance is initiated, it creates one or more tasks to stream data from Kafka to the external data system. Each task is responsible for processing a portion of the data and can run in parallel with other tasks for efficient data processing.\n",
    "4. Tasks run in parallel to poll Kafka and return records: The tasks created by the sink connector run in parallel to poll Kafka for new records. They retrieve records from Kafka topics and prepare them for forwarding to the external data system. The parallel processing of tasks enables high throughput and efficient data streaming.\n",
    "5. Converters put the records into a format suitable for the external data system: Before forwarding the records to the external data system, converters are used to put the records into a format that is suitable for the specific requirements of the external data system. Converters handle data format conversion, such as from Kafka's binary format to a format supported by the external data system.\n",
    "6. Transforms adjust the records, such as filtering or relabeling them: Depending on the configuration of the sink connector, transformations, also known as Single Message Transforms (SMTs), can be applied to adjust the records before they are forwarded to the external data system. Transformations can be used for tasks such as filtering, relabeling, or enriching the data to be sent to the external system.\n",
    "7. The sink connector is managed using KafkaConnectors or the Kafka Connect API: The sink connector, along with its tasks, is managed using KafkaConnectors, or through the Kafka Connect API, which provides programmatic access for managing Kafka Connect. This allows for easy configuration, monitoring, and management of sink connectors and their tasks in a Kafka Connect deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536c7a6f",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "We will create a simple example which will perform the following steps\n",
    "\n",
    "1. Create a Producer that will stream data from MinIO and produce events for a topic in JSON format\n",
    "2. Build a Kafka Connect Image that has S3 dependencies\n",
    "3. Deploy the Kafka Connect based on the above image\n",
    "4. Deploy Kafka sink connector that consumes kafka topic and stores the data MinIO bucket\n",
    "\n",
    "#### Getting Demo Data into MinIO\n",
    "We will be using the NYC Taxi dataset that is available on MinIO. If you don't have the dataset follow the instructions [here](../spark/spark-with-minio.ipynb#Getting-Demo-Data-into-MinIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72839ed7",
   "metadata": {},
   "source": [
    "### Producer\n",
    "\n",
    "Below is a simple python code that consumes data from MinIO and produces events for the topic `my-topic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8331553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sample-code/src/producer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sample-code/producer/src/producer.py\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import fsspec\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=\"my-kafka-cluster-kafka-bootstrap:9092\")\n",
    "\n",
    "fsspec.config.conf = {\n",
    "    \"s3\":\n",
    "        {\n",
    "            \"key\": os.getenv(\"AWS_ACCESS_KEY_ID\", \"openlakeuser\"),\n",
    "            \"secret\": os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"openlakeuser\"),\n",
    "            \"client_kwargs\": {\n",
    "                \"endpoint_url\": \"https://play.min.io:50000\"\n",
    "            }\n",
    "        }\n",
    "}\n",
    "s3 = s3fs.S3FileSystem()\n",
    "total_processed = 0\n",
    "i = 1\n",
    "for df in pd.read_csv('s3a://openlake/spark/sample-data/taxi-data.csv', chunksize=1000):\n",
    "    count = 0\n",
    "    for index, row in df.iterrows():\n",
    "        producer.send(\"my-topic\", bytes(row.to_json(), 'utf-8'))\n",
    "        count += 1\n",
    "    producer.flush()\n",
    "    total_processed += count\n",
    "    if total_processed % 10000 * i == 0:\n",
    "        logging.info(f\"total processed till now {total_processed}\")\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d611459",
   "metadata": {},
   "source": [
    "add requirements and Dockerfile based on which we will build the docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cff0452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sample-code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile sample-code/producer/requirements.txt\n",
    "pandas\n",
    "s3fs\n",
    "pyarrow\n",
    "kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19b3e53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sample-code/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile sample-code/producer/Dockerfile\n",
    "FROM python:3.11-slim\n",
    "\n",
    "ENV PYTHONDONTWRITEBYTECODE=1\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip3 install -r requirements.txt\n",
    "\n",
    "COPY src/producer.py .\n",
    "CMD [\"python3\", \"-u\", \"./producer.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f1bffc",
   "metadata": {},
   "source": [
    "Build and push the docker image for the producer using the above docker file into your docker registry or you can use the one available in openlake [openlake/kafka-demo-producer](https://hub.docker.com/r/openlake/kafka-demo-producer/tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55538acd",
   "metadata": {},
   "source": [
    "Let's create a YAML that deploys our producer in kubernetes cluster as a job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d4600d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing deployment/producer.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile deployment/producer.yaml\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: producer-job\n",
    "  namespace: kafka\n",
    "spec:\n",
    "  template:\n",
    "    metadata:\n",
    "      name: producer-job\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: producer-job\n",
    "        image: openlake/kafka-demo-producer:latest\n",
    "      restartPolicy: Never"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841f8f1a",
   "metadata": {},
   "source": [
    "Deploy the `producer.yaml` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5fb40477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job.batch/producer-job created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f deployment/producer.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d702286b",
   "metadata": {},
   "source": [
    "You can check the logs by using the below command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d73b296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<jemalloc>: MADV_DONTNEED does not work (memset will be used instead)\n",
      "<jemalloc>: (This is the expected behaviour if you are running under QEMU)\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=my-kafka-cluster-kafka-bootstrap:9092 <connecting> [IPv4 ('10.96.4.95', 9092)]>: connecting to my-kafka-cluster-kafka-bootstrap:9092 [('10.96.4.95', 9092) IPv4]\n",
      "INFO:kafka.conn:Probing node bootstrap-0 broker version\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=my-kafka-cluster-kafka-bootstrap:9092 <connecting> [IPv4 ('10.96.4.95', 9092)]>: Connection complete.\n",
      "INFO:kafka.conn:Broker version identified as 2.5.0\n",
      "INFO:kafka.conn:Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup\n",
      "INFO:kafka.conn:<BrokerConnection node_id=0 host=my-kafka-cluster-kafka-0.my-kafka-cluster-kafka-brokers.kafka.svc:9092 <connecting> [IPv4 ('10.244.1.4', 9092)]>: connecting to my-kafka-cluster-kafka-0.my-kafka-cluster-kafka-brokers.kafka.svc:9092 [('10.244.1.4', 9092) IPv4]\n",
      "INFO:kafka.conn:<BrokerConnection node_id=0 host=my-kafka-cluster-kafka-0.my-kafka-cluster-kafka-brokers.kafka.svc:9092 <connecting> [IPv4 ('10.244.1.4', 9092)]>: Connection complete.\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=my-kafka-cluster-kafka-bootstrap:9092 <connected> [IPv4 ('10.96.4.95', 9092)]>: Closing connection. \n",
      "INFO:root:total processed till now 10000\n",
      "rpc error: code = NotFound desc = an error occurred when try to find container \"85acfb121b7b63bf0f46d9ef89aed9b05666b3fb86b4a835e9d2ebf67c6943f9\": not found"
     ]
    }
   ],
   "source": [
    "!kubectl logs -f job.batch/producer-job -n kafka # stop this shell once you are done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8ce514",
   "metadata": {},
   "source": [
    "Now that we have our basic producer that is sending JSON events to `my-topic` let deploy the Kafka Connect and the corresponding Connector that stores these events into MinIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e69047",
   "metadata": {},
   "source": [
    "### Build Kafka Connect Image\n",
    "\n",
    "Lets build a kafka connect image that has S3 dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ae535be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sample-code/connect/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile sample-code/connect/Dockerfile\n",
    "FROM confluentinc/cp-kafka-connect:7.0.9 as cp\n",
    "RUN confluent-hub install --no-prompt confluentinc/kafka-connect-s3:10.4.2\n",
    "RUN confluent-hub install --no-prompt confluentinc/kafka-connect-avro-converter:7.3.3\n",
    "FROM quay.io/strimzi/kafka:0.34.0-kafka-3.4.0\n",
    "USER root:root\n",
    "# Add S3 dependency\n",
    "COPY --from=cp /usr/share/confluent-hub-components/confluentinc-kafka-connect-s3/ /opt/kafka/plugins/kafka-connect-s3/\n",
    "# Add Avro dependency\n",
    "COPY --from=cp /usr/share/confluent-hub-components/confluentinc-kafka-connect-avro-converter/ /opt/kafka/plugins/avro/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcf6d85",
   "metadata": {},
   "source": [
    "Build and push the docker image for the producer using the above docker file into your docker registry or you can use the one available in openlake [openlake/kafka-connect:0.34.0](https://hub.docker.com/r/openlake/kafka-connect/tags)\n",
    "\n",
    "Before we deploy the `KafkaConnect` we first need to create storage topcis if not already present for the KafkaConnect to work as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b862b343",
   "metadata": {},
   "source": [
    "### Create Storage Topics\n",
    "\n",
    "Lets create `connect-status`, `connect-configs` and `connect-offsets` topics and deploy them as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4f594674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing deployment/connect-status-topic.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile deployment/connect-status-topic.yaml\n",
    "apiVersion: kafka.strimzi.io/v1beta2\n",
    "kind: KafkaTopic\n",
    "metadata:\n",
    "  name: connect-status\n",
    "  namespace: kafka\n",
    "  labels:\n",
    "    strimzi.io/cluster: my-kafka-cluster\n",
    "spec:\n",
    "  partitions: 1\n",
    "  replicas: 3\n",
    "  config:\n",
    "    cleanup.policy: compact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "644eca84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing deployment/connect-configs-topic.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile deployment/connect-configs-topic.yaml\n",
    "apiVersion: kafka.strimzi.io/v1beta2\n",
    "kind: KafkaTopic\n",
    "metadata:\n",
    "  name: connect-configs\n",
    "  namespace: kafka\n",
    "  labels:\n",
    "    strimzi.io/cluster: my-kafka-cluster\n",
    "spec:\n",
    "  partitions: 1\n",
    "  replicas: 3\n",
    "  config:\n",
    "    cleanup.policy: compact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7385f700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing deployment/connect-offsets-topic.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile deployment/connect-offsets-topic.yaml\n",
    "apiVersion: kafka.strimzi.io/v1beta2\n",
    "kind: KafkaTopic\n",
    "metadata:\n",
    "  name: connect-offsets\n",
    "  namespace: kafka\n",
    "  labels:\n",
    "    strimzi.io/cluster: my-kafka-cluster\n",
    "spec:\n",
    "  partitions: 1\n",
    "  replicas: 3\n",
    "  config:\n",
    "    cleanup.policy: compact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1214a8ea",
   "metadata": {},
   "source": [
    "Deploy above topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b2566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f deployment/connect-status-topic.yaml\n",
    "!kubectl apply -f deployment/connect-configs-topic.yaml\n",
    "!kubectl apply -f deployment/connect-offsets-topic.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94afba83",
   "metadata": {},
   "source": [
    "### Deploy Kafka Connect\n",
    "\n",
    "Create a YAML file for Kafka Connect that uses the above image and deploy it in k8s. The KafkaConnect will have 1 replica and make use of ths storage topics that we created above.\n",
    "\n",
    "NOTE: `spec.template.connectContainer.env` has the creds defiend in order for KafkaConnect to store data in Minio cluster, other details like the `endpoint_url`, `bucket_name` will be part of `KafkaConnector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6d5a94fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing deployment/connect.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile deployment/connect.yaml\n",
    "apiVersion: kafka.strimzi.io/v1beta2\n",
    "kind: KafkaConnect\n",
    "metadata:\n",
    "  name: connect-cluster\n",
    "  namespace: kafka\n",
    "  annotations:\n",
    "    strimzi.io/use-connector-resources: \"true\"\n",
    "spec:\n",
    "  image: openlake/kafka-connect:0.34.0\n",
    "  version: 3.4.0\n",
    "  replicas: 1\n",
    "  bootstrapServers: my-kafka-cluster-kafka-bootstrap:9093\n",
    "  tls:\n",
    "    trustedCertificates:\n",
    "      - secretName: my-kafka-cluster-cluster-ca-cert\n",
    "        certificate: ca.crt\n",
    "  config:\n",
    "    bootstrap.servers: my-kafka-cluster-kafka-bootstrap:9092\n",
    "    group.id: connect-cluster\n",
    "    key.converter: org.apache.kafka.connect.json.JsonConverter\n",
    "    value.converter: org.apache.kafka.connect.json.JsonConverter\n",
    "    internal.key.converter: org.apache.kafka.connect.json.JsonConverter\n",
    "    internal.value.converter: org.apache.kafka.connect.json.JsonConverter\n",
    "    key.converter.schemas.enable: false\n",
    "    value.converter.schemas.enable: false\n",
    "    offset.storage.topic: connect-offsets\n",
    "    offset.storage.replication.factor: 1\n",
    "    config.storage.topic: connect-configs\n",
    "    config.storage.replication.factor: 1\n",
    "    status.storage.topic: connect-status\n",
    "    status.storage.replication.factor: 1\n",
    "    offset.flush.interval.ms: 10000\n",
    "    plugin.path: /opt/kafka/plugins\n",
    "    offset.storage.file.filename: /tmp/connect.offsets\n",
    "  template:\n",
    "    connectContainer:\n",
    "      env:\n",
    "        - name: AWS_ACCESS_KEY_ID\n",
    "          value: \"openlakeuser\"\n",
    "        - name: AWS_SECRET_ACCESS_KEY\n",
    "          value: \"openlakeuser\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2444dd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kafkaconnect.kafka.strimzi.io/connect-cluster created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f deployment/connect.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f1ad8a",
   "metadata": {},
   "source": [
    "### Deploy Kafka Sink Connector\n",
    "\n",
    "Now that we have the Kafka Connect up and running next step is to deploy the sink connector that will poll `my-topic` and store the data into MinIO bucket `openlake-tmp`.\n",
    "\n",
    "\n",
    "`connector.class` - specifies what type of connector the sink connector will use in our case it is `io.confluent.connect.s3.S3SinkConnector`\n",
    "`store.url` - MinIO endpoint URL where you want to store the data from KafkaConnect\n",
    "`storage.class` - specifies which storage class to use in our case since we are storing in MinIO `io.confluent.connect.s3.storage.S3Storage` will be used\n",
    "`format.class` -  Format type in which the data will be stored into MinIO, since we would like to store JSON we will use `io.confluent.connect.s3.format.json.JsonFormat` implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "51627425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting deployment/connector.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile deployment/connector.yaml\n",
    "apiVersion: kafka.strimzi.io/v1beta2\n",
    "kind: KafkaConnector\n",
    "metadata:\n",
    "  name: \"minio-connector\"\n",
    "  namespace: \"kafka\"\n",
    "  labels:\n",
    "    strimzi.io/cluster:\n",
    "      connect-cluster\n",
    "spec:\n",
    "  class: io.confluent.connect.s3.S3SinkConnector\n",
    "  config:\n",
    "    connector.class: io.confluent.connect.s3.S3SinkConnector\n",
    "    task.max: '1'\n",
    "    topics: my-topic\n",
    "    s3.region: us-east-1\n",
    "    s3.bucket.name: openlake-tmp\n",
    "    s3.part.size: '5242880'\n",
    "    flush.size: '1000'\n",
    "    store.url: https://play.min.io:50000\n",
    "    storage.class: io.confluent.connect.s3.storage.S3Storage\n",
    "    format.class: io.confluent.connect.s3.format.json.JsonFormat\n",
    "    partitioner.class: io.confluent.connect.storage.partitioner.DefaultPartitioner\n",
    "    behavior.on.null.values: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "610743bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kafkaconnector.kafka.strimzi.io/minio-connector created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f deployment/connector.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94416278",
   "metadata": {},
   "source": [
    "If all goes well we can see files being added to Minio `openlake-tmp` bucket by executing the below command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ba4a50a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b]11;?\u001b\\\u001b[6n\u001b[m\u001b[32m[2023-04-11 19:53:29 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000000000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:30 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000001000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:31 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000002000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:31 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000003000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:31 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000004000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:32 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000005000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:32 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000006000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:33 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000007000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:33 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000008000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:34 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000009000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:34 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000010000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:34 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000011000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:35 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000012000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:35 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000013000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:35 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000014000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:36 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000015000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:36 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000016000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:36 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000017000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:37 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000018000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:37 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000019000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:37 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000020000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:38 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000021000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:38 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000022000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:38 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000023000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:39 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000024000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:39 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000025000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:39 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000026000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:40 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000027000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:41 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000028000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:41 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000029000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:41 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000030000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:42 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000031000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:42 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000032000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:42 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000033000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:43 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000034000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:43 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000035000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:43 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000036000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:44 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000037000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:44 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000038000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:44 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000039000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:45 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000040000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:45 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000041000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:45 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000042000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:45 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000043000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:46 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000044000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:46 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000045000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:46 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000046000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:47 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000047000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:47 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000048000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:48 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000049000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:48 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000050000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:48 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000051000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:48 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000052000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:49 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000053000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:49 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000054000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:49 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000055000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:50 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000056000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:50 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000057000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:50 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000058000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:51 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000059000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:51 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000060000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:51 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000061000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:52 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000062000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:52 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000063000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:52 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000064000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:53 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000065000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:53 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000066000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:53 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000067000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:53 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000068000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:54 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000069000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:54 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000070000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:54 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000071000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:55 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000072000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:55 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000073000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:55 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000074000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:56 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000075000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:56 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000076000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:57 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000077000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:57 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000078000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:57 PDT]\u001b[0m\u001b[33m 367KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000079000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:58 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000080000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:58 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000081000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:58 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000082000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:59 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000083000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:59 PDT]\u001b[0m\u001b[33m 367KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000084000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:59 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000085000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:53:59 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000086000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:00 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000087000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:00 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000088000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:00 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000089000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:01 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000090000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:01 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000091000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:02 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000092000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:02 PDT]\u001b[0m\u001b[33m 367KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000093000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:02 PDT]\u001b[0m\u001b[33m 367KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000094000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:03 PDT]\u001b[0m\u001b[33m 367KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000095000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:03 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000096000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:03 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000097000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:03 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000098000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:04 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000099000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:04 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000100000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:04 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000101000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:05 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000102000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:05 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000103000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:05 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000104000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:05 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000105000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:06 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000106000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:06 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000107000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:06 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000108000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:07 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000109000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:07 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000110000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:07 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000111000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:07 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000112000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:08 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000113000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2023-04-11 19:54:08 PDT]\u001b[0m\u001b[33m 368KiB\u001b[0m \u001b[34mSTANDARD\u001b[0m\u001b[1m partition=0/my-topic+0+0000114000.json\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[1m\n",
      "Total Size: 41 MiB\u001b[0m\n",
      "\u001b[1mTotal Objects: 115\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!mc ls --summarize --recursive play/openlake-tmp/topics/my-topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e660164f",
   "metadata": {},
   "source": [
    "We have an end-to-end implementation of producing topics in kafka and consuming it directly into MinIO using the Kafka Connectors. There is a lot more efficient and performant way to stream and store data into MinIO from Kafka which we will conver in the next Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
