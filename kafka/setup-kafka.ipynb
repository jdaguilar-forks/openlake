{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Kafka in Kubernetes\n",
    "\n",
    "\n",
    "Kafka is an open-source distributed event streaming platform that is used for building real-time data pipelines and streaming applications. It was originally developed by LinkedIn and is now maintained by the Apache Software Foundation. Kafka is designed to handle high volume, high throughput, and low latency data streams, making it a popular choice for building scalable and reliable data streaming solutions.\n",
    "\n",
    "Some of the benefits of Kafka include:\n",
    "\n",
    "* Kafka is designed to handle large-scale data streams and can handle millions of events per second, making it highly scalable. It can be easily scaled horizontally by adding more Kafka brokers to the cluster to accommodate increasing data volumes\n",
    "* Kafka provides fault tolerance by replicating data across multiple brokers in a Kafka cluster. This ensures that data is highly available and can be recovered in case of any failures, making Kafka a reliable choice for critical data streaming applications\n",
    "* Kafka supports a variety of data sources and data sinks, making it highly versatile. It can be used for building a wide range of applications, such as real-time data processing, data ingestion, data streaming, and event-driven architectures\n",
    "* Kafka stores all published messages for a configurable amount of time, allowing consumers to read data at their own pace. This makes Kafka suitable for use cases where data needs to be retained for historical analysis or replayed for recovery purposes\n",
    "\n",
    "Deploying Kafka on Kubernetes, a widely-used container orchestration platform, offers several additional advantages. Kubernetes enables dynamic scaling of Kafka clusters based on demand, allowing for efficient resource utilization and automatic scaling of Kafka brokers to handle changing data stream volumes. This ensures that Kafka can handle varying workloads without unnecessary resource wastage or performance degradation. It provides easy deployment, management, and monitoring of Kafka clusters as containers, making them highly portable across different environments and enabling faster deployment and updates. This allows for seamless migration of Kafka clusters across various cloud providers, data centers, or development environments. Fruther, Kubernetes includes built-in features for handling failures and ensuring high availability of Kafka clusters. For example, it automatically reschedules failed Kafka broker containers and supports rolling updates without downtime, ensuring continuous availability of Kafka for data streaming applications, thereby enhancing the reliability and fault tolerance of Kafka deployments. Overall, running Kafka on Kubernetes provides scalability, flexibility, and high availability, making it a powerful combination for building and managing robust data streaming applications.\n",
    "\n",
    "When combined with object storage like MinIO, Kafka can offer great advantages in building data streaming solutions. MinIO is a high-performance, distributed object storage system that provides scalable and durable storage for unstructured data. When used as a data sink with Kafka, MinIO can provide reliable and scalable storage for data streams, allowing organizations to store and process large volumes of data in real-time.\n",
    "\n",
    "Some benefits of combining Kafka with MinIO include:\n",
    "\n",
    "* **Scalable Storage**: MinIO can handle large amounts of data and scale horizontally across multiple nodes, making it a perfect fit for storing data streams generated by Kafka. This allows organizations to store and process massive amounts of data in real-time, making it suitable for big data and high-velocity data streaming use cases\n",
    "* **Durability**: MinIO provides durable storage, allowing organizations to retain data for long periods of time. This is useful for scenarios where data needs to be stored for historical analysis, compliance requirements, or for data recovery purposes\n",
    "* **Fault Tolerance**: MinIO supports data replication across multiple nodes, providing fault tolerance and ensuring data durability. This complements Kafka's fault tolerance capabilities, making the overall solution highly reliable and resilient\n",
    "* **Easy Integration**: MinIO can be easily integrated with Kafka using Kafka Connect, which is Kafka's built-in framework for connecting Kafka with external systems. This makes it straightforward to stream data from Kafka to MinIO for storage, and vice versa for data retrieval, enabling seamless data flow between Kafka and MinIO\n",
    "\n",
    "\n",
    "In this notebook, we will walk through how to set up Kafka on Kubernetes using Strimzi, an open-source project that provides operators to run Apache Kafka and Apache ZooKeeper clusters on Kubernetes and OpenShift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before we start, ensure that you have the following prerequisites:\n",
    "\n",
    "* A running Kubernetes cluster\n",
    "* kubectl command-line tool\n",
    "* MinIO cluster up and running\n",
    "* mc command line tool for MinIO\n",
    "* Helm package manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Strimzi Operator\n",
    "The first step is to install the Strimzi operator on your Kubernetes cluster. The Strimzi operator manages the lifecycle of Kafka and ZooKeeper clusters on Kubernetes.\n",
    "\n",
    "Add Strimzi Helm chart repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"strimzi\" already exists with the same configuration, skipping\r\n"
     ]
    }
   ],
   "source": [
    "!helm repo add strimzi https://strimzi.io/charts/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install chart with release name `my-release`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME: my-release\r\n",
      "LAST DEPLOYED: Mon Apr 10 20:03:12 2023\r\n",
      "NAMESPACE: kafka\r\n",
      "STATUS: deployed\r\n",
      "REVISION: 1\r\n",
      "TEST SUITE: None\r\n",
      "NOTES:\r\n",
      "Thank you for installing strimzi-kafka-operator-0.34.0\r\n",
      "\r\n",
      "To create a Kafka cluster refer to the following documentation.\r\n",
      "\r\n",
      "https://strimzi.io/docs/operators/latest/deploying.html#deploying-cluster-operator-helm-chart-str\r\n"
     ]
    }
   ],
   "source": [
    "!helm install my-release strimzi/strimzi-kafka-operator --namespace=kafka --create-namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above command will install the latest version (0.34.0 at the time of this writing) of the operator in `kafka` namespace by creating it, for additional configurations refer to [this](https://github.com/strimzi/strimzi-kafka-operator/tree/main/helm-charts/helm3/strimzi-kafka-operator#configuration) page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Kafka Cluster\n",
    "\n",
    "Now that we have installed the Strimzi operator, we can create a Kafka cluster. In this example, we will create a Kafka cluster with three Kafka brokers and three ZooKeeper nodes.\n",
    "\n",
    "Lets create a YAML file as shown [here](https://github.com/strimzi/strimzi-kafka-operator/blob/main/examples/kafka/kafka-persistent.yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting deployment/kafka-cluster.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile deployment/kafka-cluster.yaml\n",
    "apiVersion: kafka.strimzi.io/v1beta2\n",
    "kind: Kafka\n",
    "metadata:\n",
    "  name: my-kafka-cluster\n",
    "  namespace: kafka\n",
    "spec:\n",
    "  kafka:\n",
    "    version: 3.4.0\n",
    "    replicas: 3\n",
    "    listeners:\n",
    "      - name: plain\n",
    "        port: 9092\n",
    "        type: internal\n",
    "        tls: false\n",
    "      - name: tls\n",
    "        port: 9093\n",
    "        type: internal\n",
    "        tls: true\n",
    "    config:\n",
    "      offsets.topic.replication.factor: 3\n",
    "      transaction.state.log.replication.factor: 3\n",
    "      transaction.state.log.min.isr: 2\n",
    "      default.replication.factor: 3\n",
    "      min.insync.replicas: 2\n",
    "      inter.broker.protocol.version: \"3.4\"\n",
    "    storage:\n",
    "      type: jbod\n",
    "      volumes:\n",
    "      - id: 0\n",
    "        type: persistent-claim\n",
    "        size: 100Gi\n",
    "        deleteClaim: false\n",
    "  zookeeper:\n",
    "    replicas: 3\n",
    "    storage:\n",
    "      type: persistent-claim\n",
    "      size: 100Gi\n",
    "      deleteClaim: false\n",
    "  entityOperator:\n",
    "    topicOperator: {}\n",
    "    userOperator: {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the cluster by deploying the YAML file, it will take sometime for the cluster to be up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kafka.kafka.strimzi.io/my-kafka-cluster created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f deployment/kafka-cluster.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the status of the cluster by running the below command,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               DESIRED KAFKA REPLICAS   DESIRED ZK REPLICAS   READY   WARNINGS\r\n",
      "my-kafka-cluster   3                        3                     True    \r\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kafka get kafka my-kafka-cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the cluster up and running, let try to produce and consume sample topic events, first lets create a kafka topic `my-topic`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Kafka Topic\n",
    "\n",
    "Create a YAML file for the kafka topic `my-topic` as shown below and apply it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting deployment/kafka-my-topic.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile deployment/kafka-my-topic.yaml\n",
    "apiVersion: kafka.strimzi.io/v1beta2\n",
    "kind: KafkaTopic\n",
    "metadata:\n",
    "  name: my-topic\n",
    "  namespace: kafka\n",
    "  labels:\n",
    "    strimzi.io/cluster: my-kafka-cluster\n",
    "spec:\n",
    "  partitions: 3\n",
    "  replicas: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kafkatopic.kafka.strimzi.io/connect-offsets created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f deployment/kafka-my-topic.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the status of the topic using the below command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME       CLUSTER            PARTITIONS   REPLICATION FACTOR   READY\r\n",
      "my-topic   my-kafka-cluster   3            3                    True\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kafka get kafkatopic my-topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce and Consume Messages\n",
    "\n",
    "With the Kafka cluster and topic set up, we can now produce and consume messages.\n",
    "\n",
    "Create a Kafka producer pod to produce messages to the my-topic topic, try the below commands in a terminal rather than executing it in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "kubectl -n kafka run kafka-producer -ti --image=quay.io/strimzi/kafka:0.34.0-kafka-3.4.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list my-kafka-cluster-kafka-bootstrap:9092 --topic my-topic\n",
    "```\n",
    "\n",
    "This will give us a prompt the send messages to the producer. In parallel we can bring the consumer to start consuming the messages that we sent to producer\n",
    "\n",
    "```shell\n",
    "kubectl -n kafka run kafka-consumer -ti --image=quay.io/strimzi/kafka:0.34.0-kafka-3.4.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server my-kafka-cluster-kafka-bootstrap:9092 --topic my-topic --from-beginning\n",
    "```\n",
    "\n",
    "The consumer will replay all the messages that we sent to producer earlier and if we add any new messages to the producer that will also start showing up at the consumer side.\n",
    "\n",
    "\n",
    "You can delete the `my-topic` by using the below command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kafkatopic.kafka.strimzi.io \"my-topic\" deleted\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kafka delete kafkatopic my-topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that the Kafka cluster up and running with dummy topic/producer/consumer next we can start consume topics directly into MinIO directly using the Kafka Connectors in the next Notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
