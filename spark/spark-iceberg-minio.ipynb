{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a40cee8",
   "metadata": {},
   "source": [
    "# Manage Iceberg Tables with Spark\n",
    "\n",
    "In this Notebook we will use MinIO as object storage for Spark jobs to manage iceberg tables. If you haven't already setup `spark-operator` in your Kubernetes environment follow [this](setup-spark-operator.ipynb) guide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd256e9",
   "metadata": {},
   "source": [
    "## Apache Iceberg\n",
    "\n",
    "Apache Iceberg is an open-source table format that allows for the efficient storage of large, slow-changing datasets in cloud storage systems such as Amazon S3, Azure Blob Storage, Google Cloud Storage and MinIO. Originally developed by Netflix, Iceberg addresses some of the limitations of existing table formats like Apache Parquet and Apache ORC.\n",
    "\n",
    "Iceberg is designed to provide a number of benefits over traditional table formats and a goto table format for Data Lake, below are some of them:\n",
    "\n",
    "* **Schema evolution**: Data lakes are often characterized by their flexibility and ability to store a wide variety of data formats. However, this flexibility can make it challenging to manage schema changes over time. Iceberg provides a way to add, remove, or modify table columns without requiring a full rewrite of the data, making it easier to evolve schemas over time\n",
    "* **Transactional writes**: In a data lake it is important to ensure that data is accurate and consistent, especially if the data is being used for business-critical purposes. Iceberg provides support for ACID transactions for write operations, ensuring that data is always in a consistent state\n",
    "* **Query isolation**: Data lakes are often used by many users or applications at the same time. Iceberg allows multiple queries to run concurrently without interfering with each other, making it possible to scale data lake usage without sacrificing performance\n",
    "* **Time travel**: In a data lake it is often useful to be able to query data as it appeared at a specific point in time. Iceberg provides a time-travel API that enables users to query data as it existed at a specific version or timestamp, making it easier to analyze historical trends or track changes over time\n",
    "* **Partition pruning**: Data lakes often contain large amounts of data, which can make querying slow and resource-intensive. Iceberg supports partitioning data by one or more columns, which can significantly improve query performance by reducing the amount of data that needs to be read\n",
    "\n",
    "Iceberg can be used with a variety of processing engines and frameworks, including Apache Spark, Dremio and Presto. It is also integrated with Apache Arrow, a cross-language in-memory data format, which enables efficient data serialization and deserialization across different processing engines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f95605",
   "metadata": {},
   "source": [
    "## Iceberg Catalogs\n",
    "\n",
    "Apache Iceberg catalog is a metadata store that contains information about tables, including their schema, location, and partitioning scheme. It is responsible for managing the lifecycle of tables, including creating, updating, and deleting them, and provides APIs for querying metadata and accessing data.\n",
    "\n",
    "Below are some of the catalogs supported by Apache Iceberg:\n",
    "* JDBC catalog\n",
    "* Hive catalog\n",
    "* Nessie Catalog\n",
    "* Hadoop catalog\n",
    "* Glue catalog\n",
    "* DynamoDB catalog\n",
    "* REST catalog\n",
    "\n",
    "To keep this walk through simple we will be using the `Hadoop Catalog` for our Iceberg tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddb27e9",
   "metadata": {},
   "source": [
    "## Getting Demo Data into MinIO\n",
    "We will be using the NYC Taxi dataset that is available on MinIO. You can download the dataset from [here](https://data.cityofnewyork.us/api/views/t29m-gskq/rows.csv?accessType=DOWNLOAD) which has ~112M rows and ~10GB in size. You can use any other dataset of your choice.and upload it to MinIO using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b5ba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mc mb play/openlake\n",
    "!mc mb play/openlake/spark\n",
    "!mc mb play/openlake/spark/sample-data\n",
    "!mc cp nyc-taxi-data.csv play/openlake/spark/sample-data/nyc-taxi-data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66fe16a",
   "metadata": {},
   "source": [
    "## Sample PySpark Application that Manages Iceberg Table\n",
    "\n",
    "This is based on the getting started with Iceberg [Notebook](https://github.com/tabular-io/docker-spark-iceberg/blob/main/spark/notebooks/Iceberg%20-%20Getting%20Started.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec16eab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sample-code/src/main-iceberg.py\n",
    "import logging\n",
    "import os\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, LongType, DoubleType, StringType\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(\"MinIOSparkJob\")\n",
    "\n",
    "\n",
    "# adding iceberg configs\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.sql.extensions\", \n",
    "         \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") # Use Iceberg with Spark\n",
    "    .set(\"spark.sql.catalog.demo\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .set(\"spark.sql.catalog.demo.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "    .set(\"spark.sql.catalog.demo.warehouse\", \"s3a://openlake/warehouse/\")\n",
    "    .set(\"spark.sql.catalog.demo.s3.endpoint\", \"https://play.min.io:50000\")\n",
    "    .set(\"spark.sql.defaultCatalog\", \"demo\") # Name of the Iceberg catalog\n",
    "    .set(\"spark.sql.catalogImplementation\", \"in-memory\")\n",
    "    .set(\"spark.sql.catalog.demo.type\", \"hadoop\") # Iceberg catalog type\n",
    "    .set(\"spark.executor.heartbeatInterval\", \"300000\")\n",
    "    .set(\"spark.network.timeout\", \"400000\")\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Disable below line to see INFO logs\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "def load_config(spark_context: SparkContext):\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", os.getenv(\"AWS_ACCESS_KEY_ID\", \"openlakeuser\"))\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\",\n",
    "                                                 os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"openlakeuser\"))\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", os.getenv(\"ENDPOINT\", \"play.min.io:50000\"))\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"true\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.attempts.maximum\", \"1\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.establish.timeout\", \"5000\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.timeout\", \"10000\")\n",
    "\n",
    "\n",
    "load_config(spark.sparkContext)\n",
    "\n",
    "# Define schema for NYC Taxi Data\n",
    "schema = StructType([\n",
    "    StructField('VendorID', LongType(), True),\n",
    "    StructField('tpep_pickup_datetime', StringType(), True),\n",
    "    StructField('tpep_dropoff_datetime', StringType(), True),\n",
    "    StructField('passenger_count', DoubleType(), True),\n",
    "    StructField('trip_distance', DoubleType(), True),\n",
    "    StructField('RatecodeID', DoubleType(), True),\n",
    "    StructField('store_and_fwd_flag', StringType(), True),\n",
    "    StructField('PULocationID', LongType(), True),\n",
    "    StructField('DOLocationID', LongType(), True),\n",
    "    StructField('payment_type', LongType(), True),\n",
    "    StructField('fare_amount', DoubleType(), True),\n",
    "    StructField('extra', DoubleType(), True),\n",
    "    StructField('mta_tax', DoubleType(), True),\n",
    "    StructField('tip_amount', DoubleType(), True),\n",
    "    StructField('tolls_amount', DoubleType(), True),\n",
    "    StructField('improvement_surcharge', DoubleType(), True),\n",
    "    StructField('total_amount', DoubleType(), True)])\n",
    "\n",
    "# Read CSV file from MinIO\n",
    "df = spark.read.option(\"header\", \"true\").schema(schema).csv(\n",
    "    os.getenv(\"INPUT_PATH\", \"s3a://openlake/spark/sample-data/taxi-data.csv\"))\n",
    "\n",
    "# Create Iceberg table \"nyc.taxis_large\" from RDD\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"nyc.taxis_large\")\n",
    "\n",
    "# Query table row count\n",
    "count_df = spark.sql(\"SELECT COUNT(*) AS cnt FROM nyc.taxis_large\")\n",
    "total_rows_count = count_df.first().cnt\n",
    "logger.info(f\"Total Rows for NYC Taxi Data: {total_rows_count}\")\n",
    "\n",
    "# Rename column \"fare_amount\" in nyc.taxis_large to \"fare\"\n",
    "spark.sql(\"ALTER TABLE nyc.taxis_large RENAME COLUMN fare_amount TO fare\")\n",
    "\n",
    "# Rename column \"trip_distance\" in nyc.taxis_large to \"distance\"\n",
    "spark.sql(\"ALTER TABLE nyc.taxis_large RENAME COLUMN trip_distance TO distance\")\n",
    "\n",
    "# Add description to the new column \"distance\"\n",
    "spark.sql(\n",
    "    \"ALTER TABLE nyc.taxis_large ALTER COLUMN distance COMMENT 'The elapsed trip distance in miles reported by the taximeter.'\")\n",
    "\n",
    "# Move \"distance\" next to \"fare\" column\n",
    "spark.sql(\"ALTER TABLE nyc.taxis_large ALTER COLUMN distance AFTER fare\")\n",
    "\n",
    "# Add new column \"fare_per_distance\" of type float\n",
    "spark.sql(\"ALTER TABLE nyc.taxis_large ADD COLUMN fare_per_distance FLOAT AFTER distance\")\n",
    "\n",
    "# Check the snapshots available\n",
    "snap_df = spark.sql(\"SELECT * FROM nyc.taxis_large.snapshots\")\n",
    "snap_df.show()  # prints all the available snapshots (1 till now)\n",
    "\n",
    "# Populate the new column \"fare_per_distance\"\n",
    "logger.info(\"Populating fare_per_distance column...\")\n",
    "spark.sql(\"UPDATE nyc.taxis_large SET fare_per_distance = fare/distance\")\n",
    "\n",
    "# Check the snapshots available\n",
    "logger.info(\"Checking snapshots...\")\n",
    "snap_df = spark.sql(\"SELECT * FROM nyc.taxis_large.snapshots\")\n",
    "snap_df.show()  # prints all the available snapshots (2 now) since previous operation will create a new snapshot\n",
    "\n",
    "# Qurey the table to see the results\n",
    "res_df = spark.sql(\"\"\"SELECT VendorID\n",
    "                            ,tpep_pickup_datetime\n",
    "                            ,tpep_dropoff_datetime\n",
    "                            ,fare\n",
    "                            ,distance\n",
    "                            ,fare_per_distance\n",
    "                            FROM nyc.taxis_large LIMIT 15\"\"\")\n",
    "res_df.show()\n",
    "\n",
    "# Delete rows from \"fare_per_distance\" based on criteria\n",
    "logger.info(\"Deleting rows from fare_per_distance column...\")\n",
    "spark.sql(\"DELETE FROM nyc.taxis_large WHERE fare_per_distance > 4.0 OR distance > 2.0\")\n",
    "spark.sql(\"DELETE FROM nyc.taxis_large WHERE fare_per_distance IS NULL\")\n",
    "\n",
    "# Check the snapshots available\n",
    "logger.info(\"Checking snapshots...\")\n",
    "snap_df = spark.sql(\"SELECT * FROM nyc.taxis_large.snapshots\")\n",
    "snap_df.show()  # prints all the available snapshots (4 now) since previous operations will create 2 new snapshots\n",
    "\n",
    "# Query table row count\n",
    "count_df = spark.sql(\"SELECT COUNT(*) AS cnt FROM nyc.taxis_large\")\n",
    "total_rows_count = count_df.first().cnt\n",
    "logger.info(f\"Total Rows for NYC Taxi Data after delete operations: {total_rows_count}\")\n",
    "\n",
    "# Partition table based on \"VendorID\" column\n",
    "logger.info(\"Partitioning table based on VendorID column...\")\n",
    "spark.sql(\"ALTER TABLE nyc.taxis_large ADD PARTITION FIELD VendorID\")\n",
    "\n",
    "# Query Metadata tables like snapshot, files, history\n",
    "logger.info(\"Querying Snapshot table...\")\n",
    "snapshots_df = spark.sql(\"SELECT * FROM nyc.taxis_large.snapshots ORDER BY committed_at\")\n",
    "snapshots_df.show()  # shows all the snapshots in ascending order of committed_at column\n",
    "\n",
    "logger.info(\"Querying Files table...\")\n",
    "files_count_df = spark.sql(\"SELECT COUNT(*) AS cnt FROM nyc.taxis_large.files\")\n",
    "total_files_count = files_count_df.first().cnt\n",
    "logger.info(f\"Total Data Files for NYC Taxi Data: {total_files_count}\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT file_path, \n",
    "                    file_format, \n",
    "                    record_count, \n",
    "                    null_value_counts, \n",
    "                    lower_bounds, \n",
    "                    upper_bounds \n",
    "                    FROM nyc.taxis_large.files LIMIT 1\"\"\").show()\n",
    "\n",
    "# Query history table\n",
    "logger.info(\"Querying History table...\")\n",
    "hist_df = spark.sql(\"SELECT * FROM nyc.taxis_large.history\")\n",
    "hist_df.show()\n",
    "\n",
    "# Time travel to initial snapshot\n",
    "logger.info(\"Time Travel to initial snapshot...\")\n",
    "snap_df = spark.sql(\"SELECT snapshot_id FROM nyc.taxis_large.history LIMIT 1\")\n",
    "spark.sql(f\"CALL demo.system.rollback_to_snapshot('nyc.taxis_large', {snap_df.first().snapshot_id})\")\n",
    "\n",
    "# Qurey the table to see the results\n",
    "res_df = spark.sql(\"\"\"SELECT VendorID\n",
    "                            ,tpep_pickup_datetime\n",
    "                            ,tpep_dropoff_datetime\n",
    "                            ,fare\n",
    "                            ,distance\n",
    "                            ,fare_per_distance\n",
    "                            FROM nyc.taxis_large LIMIT 15\"\"\")\n",
    "res_df.show()\n",
    "\n",
    "# Query history table\n",
    "logger.info(\"Querying History table...\")\n",
    "hist_df = spark.sql(\"SELECT * FROM nyc.taxis_large.history\")\n",
    "hist_df.show()  # 1 new row\n",
    "\n",
    "# Query table row count\n",
    "count_df = spark.sql(\"SELECT COUNT(*) AS cnt FROM nyc.taxis_large\")\n",
    "total_rows_count = count_df.first().cnt\n",
    "logger.info(f\"Total Rows for NYC Taxi Data after time travel: {total_rows_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade83cfc",
   "metadata": {},
   "source": [
    "### Building the Docker Image\n",
    "We will now build the docker image that contains the above python application. You can use the following Dockerfile to build the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8d4f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sample-code/Dockerfile\n",
    "FROM openlake/spark-py:3.3.2\n",
    "\n",
    "USER root\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "RUN pip3 install pyspark==3.3.2\n",
    "\n",
    "COPY src/*.py ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad862fb8",
   "metadata": {},
   "source": [
    "You can build your own docker image or use the pre-build image `openlake/sparkjob-demo:3.3.2` that is available on Docker Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d0ccf5",
   "metadata": {},
   "source": [
    "## Deploy Spark Iceberg Application\n",
    "\n",
    "We will build the spark job YAML by defining the specs and then deploy it in Kubernetes cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a6655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sample-code/spark-job/sparkjob-iceberg.yml\n",
    "apiVersion: \"sparkoperator.k8s.io/v1beta2\"\n",
    "kind: SparkApplication\n",
    "metadata:\n",
    "    name: spark-iceberg\n",
    "    namespace: spark-operator\n",
    "spec:\n",
    "    type: Python\n",
    "    pythonVersion: \"3\"\n",
    "    mode: cluster\n",
    "    image: \"openlake/sparkjob-demo:3.3.2\"\n",
    "    imagePullPolicy: Always\n",
    "    mainApplicationFile: local:///app/main-iceberg.py\n",
    "    sparkVersion: \"3.3.2\"\n",
    "    restartPolicy:\n",
    "        type: OnFailure\n",
    "        onFailureRetries: 3\n",
    "        onFailureRetryInterval: 10\n",
    "        onSubmissionFailureRetries: 5\n",
    "        onSubmissionFailureRetryInterval: 20\n",
    "    driver:\n",
    "        cores: 1\n",
    "        memory: \"1024m\"\n",
    "        labels:\n",
    "            version: 3.3.2\n",
    "        serviceAccount: my-release-spark\n",
    "        env:\n",
    "            -   name: AWS_REGION\n",
    "                value: us-east-1\n",
    "            -   name: AWS_ACCESS_KEY_ID\n",
    "                value: openlakeuser\n",
    "            -   name: AWS_SECRET_ACCESS_KEY\n",
    "                value: openlakeuser\n",
    "    executor:\n",
    "        cores: 1\n",
    "        instances: 3\n",
    "        memory: \"2048m\"\n",
    "        labels:\n",
    "            version: 3.3.2\n",
    "        env:\n",
    "            -   name: INPUT_PATH\n",
    "                value: \"s3a://openlake/spark/sample-data/taxi-data.csv\"\n",
    "            -   name: AWS_REGION\n",
    "                valueFrom:\n",
    "                    secretKeyRef:\n",
    "                        name: minio-secret\n",
    "                        key: AWS_REGION\n",
    "            -   name: AWS_ACCESS_KEY_ID\n",
    "                valueFrom:\n",
    "                    secretKeyRef:\n",
    "                        name: minio-secret\n",
    "                        key: AWS_ACCESS_KEY_ID\n",
    "            -   name: AWS_SECRET_ACCESS_KEY\n",
    "                valueFrom:\n",
    "                    secretKeyRef:\n",
    "                        name: minio-secret\n",
    "                        key: AWS_SECRET_ACCESS_KEY\n",
    "            -   name: ENDPOINT\n",
    "                valueFrom:\n",
    "                    secretKeyRef:\n",
    "                        name: minio-secret\n",
    "                        key: ENDPOINT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be54916",
   "metadata": {},
   "source": [
    "You can deploy the above `sparkjob-iceberg.yml` using the below command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc2005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f sample-code/spark-job/sparkjob-iceberg.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94337048",
   "metadata": {},
   "source": [
    "After the application is deployed, you can check the status of the application using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8891358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get sparkapplications -n spark-operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c59f15",
   "metadata": {},
   "source": [
    "You can also check the logs of the application using the following command:\n",
    "(Since we have disabled the `INFO` logs in the spark applications you may not see much activity until our application logs start showing up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4735f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!kubectl logs -f spark-iceberg-driver -n spark-operator # stop this shell once you are done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e66cf13",
   "metadata": {},
   "source": [
    "Once the application is completed, you can delete the application using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c751ad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete sparkapplications spark-iceberg -n spark-operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29597c0",
   "metadata": {},
   "source": [
    "## Code Walkthrough\n",
    "\n",
    "Now that we run the end-to-end code lets look at the code snippets in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25b02ca",
   "metadata": {},
   "source": [
    "### Setup Iceberg Properties\n",
    "\n",
    "```python\n",
    "# adding iceberg configs\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.sql.extensions\", \n",
    "         \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") # Use Iceberg with Spark\n",
    "    .set(\"spark.sql.catalog.demo\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .set(\"spark.sql.catalog.demo.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "    .set(\"spark.sql.catalog.demo.warehouse\", \"s3a://warehouse/\")\n",
    "    .set(\"spark.sql.catalog.demo.s3.endpoint\", \"https://play.min.io:50000\")\n",
    "    .set(\"spark.sql.defaultCatalog\", \"demo\") # Name of the Iceberg catalog\n",
    "    .set(\"spark.sql.catalogImplementation\", \"in-memory\")\n",
    "    .set(\"spark.sql.catalog.demo.type\", \"hadoop\") # Iceberg catalog type\n",
    "    .set(\"spark.executor.heartbeatInterval\", \"300000\")\n",
    "    .set(\"spark.network.timeout\", \"400000\")\n",
    ")\n",
    "```\n",
    "\n",
    "Above snippet instructs Spark to use Iceberg spark session extensions with the catalog `demo` defined as the default catalog which is of type `hadoop` with `S3FileIO` as the IO implementation with `https://play.min.io:50000` as the S3 Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4154ac",
   "metadata": {},
   "source": [
    "### Create Iceberg Table\n",
    "\n",
    "```python\n",
    "# Read CSV file from MinIO\n",
    "df = spark.read.option(\"header\", \"true\").schema(schema).csv(\n",
    "    os.getenv(\"INPUT_PATH\", \"s3a://openlake/spark/sample-data/taxi-data.csv\"))\n",
    "\n",
    "# Create Iceberg table \"nyc.taxis_large\" from RDD\n",
    "df.write.saveAsTable(\"nyc.taxis_large\")\n",
    "\n",
    "# Query table row count\n",
    "count_df = spark.sql(\"SELECT COUNT(*) AS cnt FROM nyc.taxis_large\")\n",
    "total_rows_count = count_df.first().cnt\n",
    "logger.info(f\"Total Rows for NYC Taxi Data: {total_rows_count}\")\n",
    "```\n",
    "\n",
    "In the Above snipped we read the `taxi-data.csv` file from Minio `https://play.min.io:50000` endpoint and save it as iceberg table `nyc.taxis_large`. After the Iceberg table is saved we use `Spark SQL` to query the `nyc.taxis_large` to get the total records present"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad8293a",
   "metadata": {},
   "source": [
    "### Schema Evolution\n",
    "\n",
    "```python\n",
    "# Rename column \"fare_amount\" in nyc.taxis_large to \"fare\"\n",
    "spark.sql(\"ALTER TABLE nyc.taxis_large RENAME COLUMN fare_amount TO fare\")\n",
    "\n",
    "# Rename column \"trip_distance\" in nyc.taxis_large to \"distance\"\n",
    "spark.sql(\"ALTER TABLE nyc.taxis_large RENAME COLUMN trip_distance TO distance\")\n",
    "\n",
    "# Add description to the new column \"distance\"\n",
    "spark.sql(\n",
    "    \"ALTER TABLE nyc.taxis_large ALTER COLUMN distance COMMENT 'The elapsed trip distance in miles reported by the taximeter.'\")\n",
    "\n",
    "# Move \"distance\" next to \"fare\" column\n",
    "spark.sql(\"ALTER TABLE nyc.taxis_large ALTER COLUMN distance AFTER fare\")\n",
    "\n",
    "# Add new column \"fare_per_distance\" of type float\n",
    "spark.sql(\"ALTER TABLE nyc.taxis_large ADD COLUMN fare_per_distance FLOAT AFTER distance\")\n",
    "\n",
    "# Check the snapshots available\n",
    "snap_df = spark.sql(\"SELECT * FROM nyc.taxis_large.snapshots\")\n",
    "snap_df.show()  # prints all the available snapshots (1 till now)\n",
    "\n",
    "# Populate the new column \"fare_per_distance\"\n",
    "logger.info(\"Populating fare_per_distance column...\")\n",
    "spark.sql(\"UPDATE nyc.taxis_large SET fare_per_distance = fare/distance\")\n",
    "\n",
    "```\n",
    "\n",
    "Above code demonstrates schema evolution by renaming, changing column types, adding new column `fare_per_distance` and populating the new column based on values from `fare` and `distance` columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a048b",
   "metadata": {},
   "source": [
    "### Deleting data from the table\n",
    "\n",
    "```python\n",
    "# Delete rows from \"fare_per_distance\" based on criteria\n",
    "logger.info(\"Deleting rows from fare_per_distance column...\")\n",
    "spark.sql(\"DELETE FROM nyc.taxis_large WHERE fare_per_distance > 4.0 OR distance > 2.0\")\n",
    "spark.sql(\"DELETE FROM nyc.taxis_large WHERE fare_per_distance IS NULL\")\n",
    "\n",
    "# Check the snapshots available\n",
    "logger.info(\"Checking snapshots...\")\n",
    "snap_df = spark.sql(\"SELECT * FROM nyc.taxis_large.snapshots\")\n",
    "snap_df.show()  # prints all the available snapshots (4 now) since previous operations will create 2 new snapshots\n",
    "\n",
    "# Query table row count\n",
    "count_df = spark.sql(\"SELECT COUNT(*) AS cnt FROM nyc.taxis_large\")\n",
    "total_rows_count = count_df.first().cnt\n",
    "logger.info(f\"Total Rows for NYC Taxi Data after delete operations: {total_rows_count}\")\n",
    "```\n",
    "\n",
    "In the above snippet we delete records from the new field `fare_per_distance` when its null or greater than `4.0` and when the `distance`field is greated than `2.0`. Once the operation is complete we query the snapshots table to see 2 new snapshots getting created. We also get the count of total records which is significantly less than what we started with (`397014` vs `112234626`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8a3735",
   "metadata": {},
   "source": [
    "### Partitioning the table\n",
    "\n",
    "```python\n",
    "# Partition table based on \"VendorID\" column\n",
    "logger.info(\"Partitioning table based on VendorID column...\")\n",
    "spark.sql(\"ALTER TABLE nyc.taxis_large ADD PARTITION FIELD VendorID\")\n",
    "```\n",
    "\n",
    "As seen above we create an new partition using the `VendorID` column. This partition will be applicable to the new rows that get inserted moving forward old data will not be impacted. We can also add partitons when we create the Iceberg table something like below\n",
    "\n",
    "```sql\n",
    "CREATE TABLE IF NOT EXISTS nyc.taxis_large (VendorID BIGINT, tpep_pickup_datetime STRING, tpep_dropoff_datetime STRING, passenger_count DOUBLE, trip_distance DOUBLE, RatecodeID DOUBLE, store_and_fwd_flag STRING, PULocationID BIGINT, DOLocationID BIGINT, payment_type BIGINT, fare_amount DOUBLE, extra DOUBLE, mta_tax DOUBLE, tip_amount DOUBLE, tolls_amount DOUBLE, improvement_surcharge DOUBLE, total_amount DOUBLE) PARTITIONED BY VendorID USING iceberg;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893171ff",
   "metadata": {},
   "source": [
    "### Metadata Tables\n",
    "\n",
    "```python\n",
    "# Query Metadata tables like snapshot, files, history\n",
    "logger.info(\"Querying Snapshot table...\")\n",
    "snapshots_df = spark.sql(\"SELECT * FROM nyc.taxis_large.snapshots ORDER BY committed_at\")\n",
    "snapshots_df.show()  # shows all the snapshots in ascending order of committed_at column\n",
    "\n",
    "logger.info(\"Querying Files table...\")\n",
    "files_count_df = spark.sql(\"SELECT COUNT(*) AS cnt FROM nyc.taxis_large.files\")\n",
    "total_files_count = files_count_df.first().cnt\n",
    "logger.info(f\"Total Data Files for NYC Taxi Data: {total_files_count}\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT file_path, \n",
    "                    file_format, \n",
    "                    record_count, \n",
    "                    null_value_counts, \n",
    "                    lower_bounds, \n",
    "                    upper_bounds \n",
    "                    FROM nyc.taxis_large.files LIMIT 1\"\"\").show()\n",
    "\n",
    "# Query history table\n",
    "logger.info(\"Querying History table...\")\n",
    "hist_df = spark.sql(\"SELECT * FROM nyc.taxis_large.history\")\n",
    "hist_df.show()\n",
    "```\n",
    "\n",
    "Iceberg has metadata tables like `snapshots, files, history` that we can query to understand what is going on behind the scenes. For instance by qurying the `snapshots`table we can see when a new snapshot was create what were the operations performed. `files` table gives us information about the data files stored in Minio record count per file, file formant etc., in `history` table we get all the info about when the snapshot was made current and who the parent is etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6a9a44",
   "metadata": {},
   "source": [
    "### Time Travel with snapshots\n",
    "\n",
    "```python\n",
    "# Time travel to initial snapshot\n",
    "logger.info(\"Time Travel to initial snapshot...\")\n",
    "snap_df = spark.sql(\"SELECT snapshot_id FROM nyc.taxis_large.history LIMIT 1\")\n",
    "spark.sql(f\"CALL demo.system.rollback_to_snapshot('nyc.taxis_large', {snap_df.first().snapshot_id})\")\n",
    "\n",
    "# Qurey the table to see the results\n",
    "res_df = spark.sql(\"\"\"SELECT VendorID\n",
    "                            ,tpep_pickup_datetime\n",
    "                            ,tpep_dropoff_datetime\n",
    "                            ,fare\n",
    "                            ,distance\n",
    "                            ,fare_per_distance\n",
    "                            FROM nyc.taxis_large LIMIT 15\"\"\")\n",
    "res_df.show()\n",
    "\n",
    "# Query history table\n",
    "logger.info(\"Querying History table...\")\n",
    "hist_df = spark.sql(\"SELECT * FROM nyc.taxis_large.history\")\n",
    "hist_df.show()  # 1 new row\n",
    "\n",
    "# Query table row count\n",
    "count_df = spark.sql(\"SELECT COUNT(*) AS cnt FROM nyc.taxis_large\")\n",
    "total_rows_count = count_df.first().cnt\n",
    "logger.info(f\"Total Rows for NYC Taxi Data after time travel: {total_rows_count}\")\n",
    "```\n",
    "\n",
    "It is possible to time travel in Iceberg using snapshots that capture the transactions made at a given instance. In the above code we query the `history` table to get the first snapshot that was ever created and do a `roll_back_to_snapshot` system call to that `snapshot_id`. Once the rollback has been performed when we query the table we can clearly see that `fare_per_distance` field is `null` and the record cound is back to `112234626` finally the `history` table has new record with the snapshot_id that we used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a1726f",
   "metadata": {},
   "source": [
    "This is a highlevel overview of what we can do with Apache Iceberg. There is also support for table `audit` and `maintenance` which we can explore later. Apache Iceberg is also adding support for `tags` and `branches` on top of `snapshots` that has a huge potential and we will explore that once fully functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a7931a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
