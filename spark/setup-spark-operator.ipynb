{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Spark Operator on Kubernetes Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark with Object Storage\n",
    "\n",
    "Apache Spark is an open-source, distributed computing system used for big data processing and analytics. It is designed to handle large-scale data processing with speed, efficiency, and ease of use. Spark provides a unified analytics engine for large-scale data processing, with support for multiple languages, including Java, Scala, Python, and R.\n",
    "\n",
    "The benefits of using Spark are numerous. Firstly, it provides a high level of parallelism, which means that it can process large amounts of data quickly and efficiently across multiple nodes in a cluster. Secondly, Spark provides a rich set of APIs for data processing, including support for SQL queries, machine learning, graph processing, and stream processing. Thirdly, Spark has a flexible and extensible architecture that allows developers to easily integrate with various data sources and other tools.\n",
    "\n",
    "When running Spark jobs, it is crucial to use a suitable storage system to store the input and output data. Object storage systems like Minio provide a highly scalable and durable storage solution that can handle large amounts of data. Minio is an open-source object storage system that can be easily deployed on-premises or in the cloud. It is S3-compatible, which means that it can be used with a wide range of tools that support the S3 API, including Spark.\n",
    "\n",
    "Using Minio with Spark provides several benefits over traditional Hadoop Distributed File System (HDFS) or other file-based storage systems. Firstly, object storage is highly scalable and can handle large amounts of data with ease. Secondly, it provides a flexible and cost-effective storage solution that can be easily integrated with other tools and systems. Thirdly, object storage provides a highly durable storage solution, with multiple copies of data stored across multiple nodes for redundancy and fault tolerance.\n",
    "\n",
    "In summary, Apache Spark is a powerful tool for big data processing and analytics, and using object storage systems like Minio can provide a highly scalable, durable, and flexible storage solution for running Spark jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Spark on Kubernetes?\n",
    "Deploying Apache Spark on Kubernetes offers several advantages over deploying it standalone. Here are some reasons why:\n",
    "\n",
    "1. Resource management: Kubernetes provides powerful resource management capabilities that can help optimize resource utilization and minimize wastage. By deploying Spark on Kubernetes, you can take advantage of Kubernetesâ€™ resource allocation and scheduling features to allocate resources to Spark jobs dynamically, based on their needs.\n",
    "2. Scalability: Kubernetes can automatically scale the resources allocated to Spark based on the workload. This means that Spark can scale up or down depending on the amount of data it needs to process, without the need for manual intervention.\n",
    "3. Fault-tolerance: Kubernetes provides built-in fault tolerance mechanisms that can help ensure the reliability of Spark clusters. If a node in the cluster fails, Kubernetes can automatically reschedule the Spark tasks to another node, ensuring that the workload is not impacted.\n",
    "4. Simplified deployment: Kubernetes offers a simplified deployment model, where you can deploy Spark using a single YAML file. This file specifies the resources required for the Spark cluster, and Kubernetes automatically handles the rest.\n",
    "5. Integration with other Kubernetes services: By deploying Spark on Kubernetes, you can take advantage of other Kubernetes services, such as monitoring and logging, to gain greater visibility into your Spark cluster's performance and health.\n",
    "\n",
    "Overall, deploying Spark on Kubernetes offers greater flexibility, scalability, and fault-tolerance than deploying it standalone. This can help organizations optimize their big data processing and analytics workloads, while reducing operational overhead and costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Spark on Kubernetes\n",
    "We will use Spark Operator to setup Spark on Kubernetes. Spark Operator is a Kubernetes controller that allows you to manage Spark applications on Kubernetes. It provides a custom resource definition (CRD) called SparkApplication, which allows you to define and run Spark applications on Kubernetes. Spark Operator also provides a web UI that allows you to easily monitor and manage Spark applications. Spark Operator is built on top of the Kubernetes Operator SDK, which is a framework for building Kubernetes operators. Spark Operator is open-source and available on [GitHub](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator). It is also available as a Helm chart, which makes it easy to deploy on Kubernetes. In this tutorial, we will use the Helm chart to deploy Spark Operator on a Kubernetes cluster.\n",
    "\n",
    "Spark Operator offers various features to simplify the management of Spark applications in Kubernetes environments. These include declarative application specification and management using custom resources, automatic submission of eligible SparkApplications, native cron support for scheduled applications, and customization of Spark pods beyond native capabilities through the mutating admission webhook.\n",
    "\n",
    "Additionally, the tool supports automatic re-submission and restart of updated SparkAppliations, as well as retries of failed submissions with linear back-off. It also provides functionality to mount local Hadoop configuration as a Kubernetes ConfigMap and automatically stage local application dependencies to Minio via [sparkctl](https://googlecloudplatform.github.io/spark-on-k8s-operator/sparkctl/). Finally, the tool supports the collection and export of application-level metrics and driver/executor metrics to Prometheus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "To follow this tutorial, you will need:\n",
    "1. A Kubernetes cluster. You can use Minikube to set up a local Kubernetes cluster on your machine.\n",
    "2. Helm, the package manager for Kubernetes. You can follow this guide to install Helm on your machine.\n",
    "3. A Minio server running on bare metal or kubernetes. You can follow [this](https://min.io/docs/minio/linux/operations/installation.html#install-and-deploy-minio) guide to install Minio on bare metal or [this](https://min.io/docs/minio/kubernetes/upstream/index.html) guide to install Minio on Kubernetes or you can use [play server](https://play.min.io/) for testing purposes.\n",
    "4. A Minio client (mc) to access the Minio server. You can follow [this](https://docs.min.io/docs/minio-client-quickstart-guide.html) guide to install mc on your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Spark Operator\n",
    "To install Spark Operator, you need to add the Helm repository for Spark Operator to your local Helm client. You can do this by running the following command:\n",
    "\n",
    "```bash\n",
    "helm repo add spark-operator https://googlecloudplatform.github.io/spark-on-k8s-operator\n",
    "```\n",
    "\n",
    "Once the repository is added, you can install Spark Operator using the following command:\n",
    "\n",
    "```bash\n",
    "helm install my-release spark-operator/spark-operator \\\n",
    "--namespace spark-operator \\\n",
    "--set webhook.enable=true \\\n",
    "--set image.repository=openlake/spark-operator \\\n",
    "--set image.tag=3.3.1 \\\n",
    "--create-namespace\n",
    "```\n",
    "\n",
    "You will see the following output:\n",
    "\n",
    "```bash\n",
    "LAST DEPLOYED: Mon Feb 27 19:48:33 2023\n",
    "NAMESPACE: spark-operator\n",
    "STATUS: deployed\n",
    "REVISION: 1\n",
    "TEST SUITE: None\n",
    "```\n",
    "\n",
    "This command installs Spark Operator in the spark-operator namespace, and enables the mutating admission webhook. The webhook is required to enable the mounting of local Hadoop configuration as a Kubernetes ConfigMap and to configure env variables that driver and executor can use. The image repository and tag are set to the image that contains the latest version of Spark Operator. You can also use the default image repository and tag by omitting the --set image.repository and --set image.tag flags, at the time of this writing the latest Spark Operator release used 3.1.1 version of Spark whereas openlake/spark-operator used the latest 3.3.1 release of Spark. You can skip the --create-namespace flag if you already have a namespace named spark-operator. This will also monitor all the spark applications in all the namespaces.\n",
    "\n",
    "Detailed list of configuration options can be found [here](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/charts/spark-operator-chart/README.md#values).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Spark Operator Installation\n",
    "To verify that Spark Operator is installed successfully, you can run the following command:\n",
    "\n",
    "```bash\n",
    "kubectl get pods -n spark-operator\n",
    "```\n",
    "\n",
    "You will see the following output:\n",
    "\n",
    "```bash\n",
    "NAME                                        READY   STATUS    RESTARTS   AGE\n",
    "my-release-spark-operator-f56c4d8c4-pr857   1/1     Running   0          14m\n",
    "```\n",
    "\n",
    "Now that we have the spark operator installed, we can deploy a Spark application or Scheduled Spark application on Kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy a Spark Application\n",
    "Let's try deploying one of the example simple spark application that comes with the spark operator. You can find the list of example applications [here](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/tree/master/examples) that calcuates Pi, we will modify the [spark Pi application](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/examples/spark-py-pi.yaml) to use Spark 3.3.1 and run it on Kubernetes.\n",
    "\n",
    "```yaml\n",
    "apiVersion: \"sparkoperator.k8s.io/v1beta2\"\n",
    "kind: SparkApplication\n",
    "metadata:\n",
    "    name: pyspark-pi\n",
    "    namespace: spark-operator\n",
    "spec:\n",
    "    type: Python\n",
    "    pythonVersion: \"3\"\n",
    "    mode: cluster\n",
    "    image: \"openlake/spark-py:3.3.1\"\n",
    "    imagePullPolicy: Always\n",
    "    mainApplicationFile: local:///opt/spark/examples/src/main/python/pi.py\n",
    "    sparkVersion: \"3.3.1\"\n",
    "    restartPolicy:\n",
    "        type: OnFailure\n",
    "        onFailureRetries: 3\n",
    "        onFailureRetryInterval: 10\n",
    "        onSubmissionFailureRetries: 5\n",
    "        onSubmissionFailureRetryInterval: 20\n",
    "    driver:\n",
    "        cores: 1\n",
    "        coreLimit: \"1200m\"\n",
    "        memory: \"512m\"\n",
    "        labels:\n",
    "            version: 3.1.1\n",
    "        serviceAccount: my-release-spark\n",
    "    executor:\n",
    "        cores: 1\n",
    "        instances: 1\n",
    "        memory: \"512m\"\n",
    "        labels:\n",
    "            version: 3.3.1\n",
    "```\n",
    "\n",
    "The above application will calculate the value of Pi using Spark on Kubernetes. You can save the above application as spark-pi.yaml and deploy it using the following command:\n",
    "\n",
    "```bash\n",
    "kubectl apply -f spark-pi.yaml\n",
    "```\n",
    "\n",
    "You will see the following output:\n",
    "\n",
    "```bash\n",
    "NAME                                           READY   STATUS      RESTARTS   AGE\n",
    "my-release-spark-operator-59bccf4d94-fdrc9     1/1     Running     0          24d\n",
    "my-release-spark-operator-webhook-init-jspnn   0/1     Completed   0          68d\n",
    "pyspark-pi-driver                              1/1     Running     0          23s\n",
    "pythonpi-b6a3e48693762e5d-exec-1               1/1     Running     0          7s\n",
    "```\n",
    "\n",
    "You can check the status of the application using the following command:\n",
    "\n",
    "```bash\n",
    "kubectl get sparkapplications -n spark-operator\n",
    "```\n",
    "\n",
    "You will see the following output:\n",
    "\n",
    "```bash\n",
    "NAME         STATUS      ATTEMPTS   START                  FINISH                 AGE\n",
    "pyspark-pi   COMPLETED   1          2023-02-27T15:20:29Z   2023-02-27T15:20:59Z   10m\n",
    "```\n",
    "\n",
    "You can also check the logs of the application using the following command:\n",
    "\n",
    "```bash\n",
    "kubectl logs pyspark-pi-driver -n spark-operator\n",
    "```\n",
    "\n",
    "You will see the following output:\n",
    "\n",
    "```bash\n",
    "23/02/27 15:20:55 INFO DAGScheduler: Job 0 finished: reduce at /opt/spark/examples/src/main/python/pi.py:42, took 2.597098 s\n",
    "Pi is roughly 3.137960\n",
    "23/02/27 15:20:55 INFO SparkUI: Stopped Spark web UI at http://pyspark-pi-d73653869375fa87-driver-svc.spark-operator.svc:4040\n",
    "23/02/27 15:20:55 INFO KubernetesClusterSchedulerBackend: Shutting down all executors\n",
    "```\n",
    "\n",
    "Now that we have the simple spark application working as expected we can try to read and write data from Minio using Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and Write Data from Minio using Spark\n",
    "\n",
    "Reading and writing Data from and to Minio using Spark is very simple once we have the right dependencies and configurations in place.\n",
    "In this post we will not be discussing the dependencies, to keep things simple we use the `openlake/spark-py:3.3.1` image that contains all the dependencies required to read and write data from Minio using Spark.\n",
    "\n",
    "#### Getting Demo Data into Minio\n",
    "We will be using the NYC Taxi dataset that is available on Minio. You can download the dataset from [here](https://data.cityofnewyork.us/api/views/t29m-gskq/rows.csv?accessType=DOWNLOAD) which has ~112M rows and ~10GB in size. You can use any other dataset of your choice.and upload it to Minio using the following command:\n",
    "\n",
    "```bash\n",
    "mc cp nyc-taxi-data.csv play/openlake/spark/sample-data/nyc-taxi-data.csv\n",
    "```\n",
    "\n",
    "#### Sample Python Application\n",
    "Let's now read and write data from Minio using Spark. We will use the following sample python application to do that.\n",
    "\n",
    "```python\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, LongType, DoubleType, StringType\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(\"MinioSparkJob\")\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "\n",
    "def load_config(spark_context: SparkContext):\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", os.getenv(\"AWS_ACCESS_KEY_ID\", \"openlakeuser\"))\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\",\n",
    "                                                 os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"openlakeuser\"))\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", os.getenv(\"ENDPOINT\", \"play.min.io:50000\"))\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"true\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.attempts.maximum\", \"1\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.establish.timeout\", \"5000\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.timeout\", \"10000\")\n",
    "\n",
    "\n",
    "load_config(spark.sparkContext)\n",
    "\n",
    "# Define schema for NYC Taxi Data\n",
    "schema = StructType([\n",
    "    StructField('VendorID', LongType(), True),\n",
    "    StructField('tpep_pickup_datetime', StringType(), True),\n",
    "    StructField('tpep_dropoff_datetime', StringType(), True),\n",
    "    StructField('passenger_count', DoubleType(), True),\n",
    "    StructField('trip_distance', DoubleType(), True),\n",
    "    StructField('RatecodeID', DoubleType(), True),\n",
    "    StructField('store_and_fwd_flag', StringType(), True),\n",
    "    StructField('PULocationID', LongType(), True),\n",
    "    StructField('DOLocationID', LongType(), True),\n",
    "    StructField('payment_type', LongType(), True),\n",
    "    StructField('fare_amount', DoubleType(), True),\n",
    "    StructField('extra', DoubleType(), True),\n",
    "    StructField('mta_tax', DoubleType(), True),\n",
    "    StructField('tip_amount', DoubleType(), True),\n",
    "    StructField('tolls_amount', DoubleType(), True),\n",
    "    StructField('improvement_surcharge', DoubleType(), True),\n",
    "    StructField('total_amount', DoubleType(), True)])\n",
    "\n",
    "# Read CSV file from Minio\n",
    "df = spark.read.option(\"header\", \"true\").schema(schema).csv(\n",
    "    os.getenv(\"INPUT_PATH\", \"s3a://openlake/spark/sample-data/taxi-data.csv\"))\n",
    "# Filter dataframe based on passenger_count greater than 6\n",
    "large_passengers_df = df.filter(df.passenger_count > 6)\n",
    "\n",
    "total_rows_count = df.count()\n",
    "filtered_rows_count = large_passengers_df.count()\n",
    "# File Output Committer is used to write the output to the destination (Not recommended for Production)\n",
    "large_passengers_df.write.format(\"csv\").option(\"header\", \"true\").save(\n",
    "    os.getenv(\"OUTPUT_PATH\", \"s3a://openlake-tmp/spark/nyc/taxis_small\"))\n",
    "\n",
    "logger.info(f\"Total Rows for NYC Taxi Data: {total_rows_count}\")\n",
    "logger.info(f\"Total Rows for Passenger Count > 6: {filtered_rows_count}\")\n",
    "```\n",
    "\n",
    "The above application reads the NYC Taxi dataset from Minio and filters the rows where the passenger count is greater than 6. The filtered data is then written to Minio. You can save the above code as `main.py`\n",
    "\n",
    "#### Building the Docker Image\n",
    "We will now build the docker image that contains the above python application. You can use the following Dockerfile to build the image:\n",
    "\n",
    "```dockerfile\n",
    "FROM openlake/spark-py:3.3.1\n",
    "\n",
    "USER root\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "RUN pip3 install pyspark==3.3.1\n",
    "\n",
    "COPY src/*.py .\n",
    "```\n",
    "You can build your own docker image or use the pre-build image `openlake/sparkjob-demo:3.3.1` that is available on Docker Hub.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploying the Minio Spark Application\n",
    "To read and write data from Minio using Spark, you need to create a secret that contains the Minio access key and secret key. You can create the secret using the following command:\n",
    "\n",
    "```bash\n",
    "kubectl create secret generic minio-secret \\\n",
    "--from-literal=AWS_ACCESS_KEY_ID=openlakeuser \\\n",
    "--from-literal=AWS_SECRET_ACCESS_KEY=openlakeuser \\\n",
    "--from-literal=ENDPOINT=minio.openlake.io \\                                                                                               --from-literal=AWS_REGION=us-east-1 \\\n",
    "--namespace spark-operator\n",
    "```\n",
    "\n",
    "You will see the following output:\n",
    "\n",
    "```bash\n",
    "secret/minio-secret created\n",
    "```\n",
    "\n",
    "Now that we have the secret created, we can deploy the spark application that reads and writes data from Minio. You can save the following application as sparkjob-minio.yaml:\n",
    "\n",
    "```yaml\n",
    "apiVersion: \"sparkoperator.k8s.io/v1beta2\"\n",
    "kind: SparkApplication\n",
    "metadata:\n",
    "    name: spark-minio\n",
    "    namespace: spark-operator\n",
    "spec:\n",
    "    type: Python\n",
    "    pythonVersion: \"3\"\n",
    "    mode: cluster\n",
    "    image: \"openlake/sparkjob-demo:3.3.1\"\n",
    "    imagePullPolicy: Always\n",
    "    mainApplicationFile: local:///app/main.py\n",
    "    sparkVersion: \"3.3.1\"\n",
    "    restartPolicy:\n",
    "        type: OnFailure\n",
    "        onFailureRetries: 3\n",
    "        onFailureRetryInterval: 10\n",
    "        onSubmissionFailureRetries: 5\n",
    "        onSubmissionFailureRetryInterval: 20\n",
    "    driver:\n",
    "        cores: 1\n",
    "        memory: \"1024m\"\n",
    "        labels:\n",
    "            version: 3.3.1\n",
    "        serviceAccount: my-release-spark\n",
    "        env:\n",
    "            -   name: AWS_REGION\n",
    "                value: us-east-1\n",
    "            -   name: AWS_ACCESS_KEY_ID\n",
    "                value: openlakeuser\n",
    "            -   name: AWS_SECRET_ACCESS_KEY\n",
    "                value: openlakeuser\n",
    "    executor:\n",
    "        cores: 1\n",
    "        instances: 3\n",
    "        memory: \"1024m\"\n",
    "        labels:\n",
    "            version: 3.3.1\n",
    "        env:\n",
    "            -   name: INPUT_PATH\n",
    "                value: \"s3a://openlake/spark/sample-data/taxi-data.csv\"\n",
    "            -   name: OUTPUT_PATH\n",
    "                value: \"s3a://openlake/spark/output/taxi-data-output\"\n",
    "            -   name: AWS_REGION\n",
    "                valueFrom:\n",
    "                    secretKeyRef:\n",
    "                        name: minio-secret\n",
    "                        key: AWS_REGION\n",
    "            -   name: AWS_ACCESS_KEY_ID\n",
    "                valueFrom:\n",
    "                    secretKeyRef:\n",
    "                        name: minio-secret\n",
    "                        key: AWS_ACCESS_KEY_ID\n",
    "            -   name: AWS_SECRET_ACCESS_KEY\n",
    "                valueFrom:\n",
    "                    secretKeyRef:\n",
    "                        name: minio-secret\n",
    "                        key: AWS_SECRET_ACCESS_KEY\n",
    "            -   name: ENDPOINT\n",
    "                valueFrom:\n",
    "                    secretKeyRef:\n",
    "                        name: minio-secret\n",
    "                        key: ENDPOINT\n",
    "```\n",
    "Above Python Spark Application YAML file contains the following configurations:\n",
    "\n",
    "* `spec.type`: The type of the application. In this case, it is a Python application.\n",
    "* `spec.pythonVersion`: The version of Python used in the application.\n",
    "* `spec.mode`: The mode of the application. In this case, it is a cluster mode application.\n",
    "* `spec.image`: The docker image that contains the application.\n",
    "* `spec.imagePullPolicy`: The image pull policy for the docker image.\n",
    "* `spec.mainApplicationFile`: The path to the main application file.\n",
    "* `spec.sparkVersion`: The version of Spark used in the application.\n",
    "* `spec.restartPolicy`: The restart policy for the application. In this case, the application will be restarted if it fails. The application will be restarted 3 times with a 10 seconds interval between each restart. If the application fails to submit, it will be restarted 5 times with a 20 seconds interval between each restart.\n",
    "* `spec.driver`: The driver configuration for the application. In this case, we are using the `my-release-spark` service account. The driver environment variables are set to read and write data from Minio.\n",
    "* `spec.executor`: The executor configuration for the application. In this case, we are using 3 executors with 1 core and 1GB of memory each. The executor environment variables are set to read and write data from Minio.\n",
    "\n",
    "\n",
    "You can deploy the application using the following command:\n",
    "\n",
    "```bash\n",
    "kubectl apply -f sparkjob-minio.yaml\n",
    "```\n",
    "\n",
    "After the application is deployed, you can check the status of the application using the following command:\n",
    "\n",
    "```bash\n",
    "kubectl get sparkapplications -n spark-operator\n",
    "```\n",
    "\n",
    "You will see the following output:\n",
    "\n",
    "```bash\n",
    "NAME          STATUS    ATTEMPTS   START                  FINISH       AGE\n",
    "spark-minio   RUNNING   1          2023-02-27T18:47:33Z   <no value>   4m4s\n",
    "```\n",
    "\n",
    "Once the application is completed, you can check the output data in Minio. You can use the following command to list the files in the output directory:\n",
    "\n",
    "```bash\n",
    "mc ls minio/openlake/spark/output/taxi-data-output\n",
    "```\n",
    "\n",
    "You can also check the logs of the application using the following command:\n",
    "\n",
    "```bash\n",
    "kubectl logs -f spark-minio-driver -n spark-operator\n",
    "```\n",
    "\n",
    "You will see the following output:\n",
    "\n",
    "```bash\n",
    "23/02/27 19:06:11 INFO FileFormatWriter: Finished processing stats for write job 91dee4ed-3f0f-4b5c-8260-bf99c0b662ba.\n",
    "2023-02-27 19:06:11,578 - MinioSparkJob - INFO - Total Rows for NYC Taxi Data: 112234626\n",
    "2023-02-27 19:06:11,578 - MinioSparkJob - INFO - Total Rows for Passenger Count > 6: 1066\n",
    "2023-02-27 19:06:11,578 - py4j.clientserver - INFO - Closing down clientserver connection\n",
    "23/02/27 19:06:11 INFO SparkUI: Stopped Spark web UI at http://spark-minio-b8d5c4869440db05-driver-svc.spark-operator.svc:4040\n",
    "23/02/27 19:06:11 INFO KubernetesClusterSchedulerBackend: Shutting down all executors\n",
    "\n",
    "```\n",
    "\n",
    "There is also option for you to use the Spark UI to monitor the application. You can use the following command to port forward the Spark UI:\n",
    "\n",
    "```bash\n",
    "kubectl port-forward svc/spark-minio-ui-svc 4040:4040 -n spark-operator\n",
    "```\n",
    "In your browser, you can access the Spark UI using the following URL: `http://localhost:4040`\n",
    "\n",
    "You will see the following Spark UI:\n",
    "\n",
    "![Spark UI](./img/spark-ui.png)\n",
    "<img src=\"img/spark-ui.png\">\n",
    "\n",
    "Once the application is completed, you can delete the application using the following command:\n",
    "\n",
    "```bash\n",
    "kubectl delete sparkapplications spark-minio -n spark-operator\n",
    "```\n",
    "\n",
    "Deploying a Scheduled Spark Application is almost the same as deploying a normal Spark Application. The only difference is that you need to add the `spec.schedule` field to the Spark Application YAML file and the kind is `ScheduledSparkApplication`. You can save the following application as sparkjob-minio-scheduled.yaml:\n",
    "\n",
    "```yaml\n",
    "apiVersion: \"sparkoperator.k8s.io/v1beta2\"\n",
    "kind: ScheduledSparkApplication\n",
    "metadata:\n",
    "    name: spark-scheduled-minio\n",
    "    namespace: spark-operator\n",
    "spec:\n",
    "    schedule: \"@every 1h\" # Run the application every hour\n",
    "    concurrencyPolicy: Allow\n",
    "    template:\n",
    "        type: Python\n",
    "        pythonVersion: \"3\"\n",
    "        mode: cluster\n",
    "        image: \"openlake/sparkjob-demo:3.3.1\"\n",
    "        imagePullPolicy: Always\n",
    "        mainApplicationFile: local:///app/main.py\n",
    "        sparkVersion: \"3.3.1\"\n",
    "        restartPolicy:\n",
    "            type: OnFailure\n",
    "            onFailureRetries: 3\n",
    "            onFailureRetryInterval: 10\n",
    "            onSubmissionFailureRetries: 5\n",
    "            onSubmissionFailureRetryInterval: 20\n",
    "        driver:\n",
    "            cores: 1\n",
    "            memory: \"1024m\"\n",
    "            labels:\n",
    "                version: 3.3.1\n",
    "            serviceAccount: my-release-spark\n",
    "            env:\n",
    "                -   name: AWS_REGION\n",
    "                    value: us-east-1\n",
    "                -   name: AWS_ACCESS_KEY_ID\n",
    "                    value: openlakeuser\n",
    "                -   name: AWS_SECRET_ACCESS_KEY\n",
    "                    value: openlakeuser\n",
    "        executor:\n",
    "            cores: 1\n",
    "            instances: 3\n",
    "            memory: \"1024m\"\n",
    "            labels:\n",
    "                version: 3.3.1\n",
    "            env:\n",
    "                -   name: INPUT_PATH\n",
    "                    value: \"s3a://openlake/spark/sample-data/taxi-data.csv\"\n",
    "                -   name: OUTPUT_PATH\n",
    "                    value: \"s3a://openlake/spark/output/taxi-data-output\"\n",
    "                -   name: AWS_REGION\n",
    "                    valueFrom:\n",
    "                        secretKeyRef:\n",
    "                            name: minio-secret\n",
    "                            key: AWS_REGION\n",
    "                -   name: AWS_ACCESS_KEY_ID\n",
    "                    valueFrom:\n",
    "                        secretKeyRef:\n",
    "                            name: minio-secret\n",
    "                            key: AWS_ACCESS_KEY_ID\n",
    "                -   name: AWS_SECRET_ACCESS_KEY\n",
    "                    valueFrom:\n",
    "                        secretKeyRef:\n",
    "                            name: minio-secret\n",
    "                            key: AWS_SECRET_ACCESS_KEY\n",
    "                -   name: ENDPOINT\n",
    "                    valueFrom:\n",
    "                        secretKeyRef:\n",
    "                            name: minio-secret\n",
    "                            key: ENDPOINT\n",
    "```\n",
    "\n",
    "You can deploy and see the results of the application in the same way as the normal Spark Application. Above Spark Application will run every hour and will write the output to the same directory in Minio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
