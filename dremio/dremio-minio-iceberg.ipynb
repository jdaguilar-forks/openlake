{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f155174",
   "metadata": {},
   "source": [
    "# Dremio MinIO Iceberg\n",
    "\n",
    "\n",
    "In this notebook we will setup Dremio to read files like CSV from Minio, we will also access Apache Iceberg table that was created using Spark as shown [here](../spark/spark-iceberg-minio.ipynb). If you haven't setup Dremio yet follow the walkthrough in [this Notebook](setup-dremio.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e02efa9",
   "metadata": {},
   "source": [
    "## Add MinIO as Datasource\n",
    "\n",
    "Once we are login to Dremio, lets click on `Add Source` at the bottom left\n",
    "\n",
    "![add_source](./img/add-source.png)\n",
    "\n",
    "Select `Amazon S3` under `Object Storage`\n",
    "\n",
    "![select_s3](./img/select-S3.png)\n",
    "\n",
    "Fill in the details like `Name` of the connector `AWS Access Key`, `AWS Access Secret` and add `Buckets` in our case `openlake` as shown below\n",
    "\n",
    "![s3-details](./img/s3-details.png)\n",
    "\n",
    "Next we choose the `Advanced Options` on the left side of the menu enable `Enable compatibility mode` and add 2 new `Connection Properties`\n",
    "* fs.s3a.endpoint - play.min.io\n",
    "* fs.s3a.path.style.access - true\n",
    "\n",
    "Add `openlake` to the `Allowlisted buckets` and hit `save` as shown in the image below\n",
    "\n",
    "![s3-details2](./img/s3-details2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1f2578",
   "metadata": {},
   "source": [
    "## Accessing CSV file\n",
    "\n",
    "Let's use the `taxi-data.csv` that we used in spark notebooks, if not you can follow the instructions [here](../spark/spark-iceberg-minio.ipynb#Getting-Demo-Data-into-MinIO) to get the data into MinIO. Click on the `openlake` datasource that we just setup\n",
    "\n",
    "![source](./img/source.png)\n",
    "\n",
    "Navigate to `openlake/spark/sample-data` you should see `taxi-data.csv` file there click on the `Format File` as shown below\n",
    "\n",
    "![format-file](./img/format-file.png)\n",
    "\n",
    "Dremio should be able to infer the schema of the CSV file but do some changes as shown below\n",
    "\n",
    "![schema](./img/schema.png)\n",
    "\n",
    "Click on `Save` and you should be navigated to the SQL editor, lets run a simple query to see the data\n",
    "\n",
    "```sql\n",
    "SELECT count(*) FROM openlake.openlake.spark.\"sample-data\".\"taxi-data.csv\";\n",
    "```\n",
    "\n",
    "It will take sometime to load the data and compute the count, once done you should see the result as shown below\n",
    "\n",
    "![count](./img/count.png)\n",
    "Note: It took approx more than 2 mins to complete the above query, the time taken depends on the size of the data and the compute resources available to Dremio.\n",
    "\n",
    "We can perform other query operations but we will not be able to alter the column names or time travel to previous versions of the data. We will use Apache Iceberg to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Accessing Iceberg table\n",
    "\n",
    "We will use the `nyc.taxis_large` Iceberg table that we created in [this Notebook](../spark/spark-iceberg-minio.ipynb) to access the data using Dremio. Click on the `openlake` datasource that we just setup\n",
    "\n",
    "![source](./img/source.png)\n",
    "\n",
    "Navigate to `openlake/warehouse/nyc` you should see `taxis_large` there click on the `Format File` Dremio should be able to infer the schema of the Iceberg table as shown below\n",
    "\n",
    "![iceberg_schema](./img/iceberg-schema.png)\n",
    "\n",
    "Click on `Save` and you should be navigated to the SQL editor, lets run a simple query to see the data\n",
    "\n",
    "```sql\n",
    "SELECT count(*) FROM openlake.openlake.warehouse.nyc.taxis_large;\n",
    "```\n",
    "The query execution time is much faster than the CSV file `<1s` as shown below\n",
    "![count_iceberg](./img/count-iceberg.png)\n",
    "\n",
    "Note: we are seeing all the records in the table since we switched to the earliest snapshot of the table via time travel from the `spark-iceberg-minio` [notebook](../spark/spark-iceberg-minio.ipynb#T#Time-Travel-with-snapshots)\n",
    "\n",
    "At the time of this writing Dremio does not support time travel via snapshots or to access any metadata tables, so we will not be able to access the data in the previous snapshots. We can use processing engines like Spark to access the data in previous snapshots.\n",
    "\n",
    "### Update Table Operations\n",
    "Let's try to populate the `fare_per_distance` column by dividing the `fare` by `trip` and see if we can query the data, You can execute the following queries in the SQL editor all at once\n",
    "\n",
    "```sql\n",
    "SELECT VendorID\n",
    "        ,tpep_pickup_datetime\n",
    "        ,tpep_dropoff_datetime\n",
    "        ,fare\n",
    "        ,distance\n",
    "        ,fare_per_distance\n",
    "        FROM openlake.openlake.warehouse.nyc.taxis_large LIMIT 15;\n",
    "\n",
    "UPDATE openlake.openlake.warehouse.nyc.taxis_large SET fare_per_distance = fare/distance WHERE distance > 0;\n",
    "\n",
    "SELECT VendorID\n",
    "        ,tpep_pickup_datetime\n",
    "        ,tpep_dropoff_datetime\n",
    "        ,fare\n",
    "        ,distance\n",
    "        ,fare_per_distance\n",
    "        FROM openlake.openlake.warehouse.nyc.taxis_large LIMIT 15;\n",
    "```\n",
    "Above we are first fetching first 15 records from the Iceberg table there we will notice `fare_per_distance` is `null`, then we execute the `UPDATE` query to populate the `fare_per_distance` column and finally we fetch the first 15 records again and we will notice the `fare_per_distance` column is populated.\n",
    "\n",
    "![query-1](./img/query-1-iceberg.png)\n",
    "\n",
    "![query-2](./img/query-2-iceberg.png)\n",
    "\n",
    "![query-3](./img/query-3-iceberg.png)\n",
    "\n",
    "\n",
    "### Delete Table Operations\n",
    "Let's try to delete some records from the table, You can execute the following queries in the SQL editor all at once\n",
    "\n",
    "```sql\n",
    "DELETE FROM openlake.openlake.warehouse.nyc.taxis_large WHERE fare_per_distance > 4.0 OR distance > 2.0;\n",
    "DELETE FROM openlake.openlake.warehouse.nyc.taxis_large WHERE fare_per_distance IS NULL;\n",
    "\n",
    "SELECT count(*) FROM openlake.openlake.warehouse.nyc.taxis_large;\n",
    "```\n",
    "\n",
    "![query-4](./img/query-4-iceberg.png)\n",
    "\n",
    "![query-5](./img/query-5-iceberg.png)\n",
    "\n",
    "![query-6](./img/query-6-iceberg.png)\n",
    "\n",
    "Above we are deleting records where `fare_per_distance` is greater than 4.0 or `distance` is greater than 2.0 and then we are deleting records where `fare_per_distance` is `null` and finally we are counting the number of records in the table."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Monitor Jobs\n",
    "\n",
    "We can monitor the jobs that are running or already completed in Dremio by clicking on the `Jobs` tab on the left side of the menu as shown below\n",
    "\n",
    "![jobs](./img/jobs.png)\n",
    "Jobs can be filtered by based on various parameters depending on the user need.\n",
    "\n",
    "We can also see the details of the job like the memory usage breakdown of the total execution time etc., by clicking on the job name as shown below.\n",
    "![job-details](./img/job-details.png)\n",
    "\n",
    "\n",
    "We have now setup Dremio to read data from MinIO and access Iceberg table created using Spark."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
